% !TEX root = ../thesis.tex

\begin{figure}

\centering
\begin{tikzpicture}
  [
    state/.style={draw, rectangle, minimum width=6mm, minimum height=6mm, inner sep=2pt},
    func/.style={draw, circle, minimum width=1cm, minimum height=1cm, inner sep=2pt},
    loss/.style={draw, diamond, minimum width=1cm, minimum height=1cm, inner sep=2pt},
    node distance=20mm,
    classical/.style={dashed,->,shorten >=4pt,shorten <=4pt,>=stealth},
%     dotted/.style={dotted,-,shorten >=4pt,shorten <=4pt,>=stealth}
  ]
  \node (x) [state, label=left:Input] {$x$};
  \node (f1) [func, below of=x,label=left:Encoder 1] {$f_1(x)$};
  \node(h1)[state, below of=f1]{$h_1$};
  \node (f2) [func, below of=h1,label=left:Encoder 2] {$f_2(x)$};
  \node (fn) [func, below of=f2,label=left:Encoder $n$] {$f_n(x)$};

  \node(hn)[state, below of=fn, right of=fn,label=left:Features]{$h_n$};

  \node(gn)[func, above of=hn, right of=hn,label=right:Decoder $n$]{$g_n(h)$};
  \node(g2)[func, above of=gn, label=right:Decoder 2]{$g_2(h)$};
  \node(y1)[state, above of=g2]{$\hat{h}_1$};
  \node(g1)[func, above of=y1,label=right:Decoder 1]{$g_1(h)$};
  \node(y)[state, above of=g1, label=right:Reconstruction]{$y$};

  \node(l1)[loss, right of=x, label=below:Loss]{$L2$};
  \node(l2)[loss, right of=h1, label=below:Loss]{$L2_2$};
  \node(lnce)[loss, below of=hn, label=below:Sparsity penalty]{$L_{NCE}$};

  \draw [->] (x) -- (f1);
  \draw [->] (f1) -- (h1);
  \draw [->] (h1) -- (f2);
  \draw [dotted] (f2) -- (fn);
  \draw [->] (fn) -- (hn);
  \draw [->] (hn) -- (gn);
  \draw [dotted] (gn) -- (g2);
  \draw [->] (g2) -- (y1);
  \draw [->] (y1) -- (g1);
  \draw [->] (g1) -- (y);

  \draw [classical] (l1) -- (x);
  \draw [classical] (l1) -- (y);

  \draw [classical] (l2) -- (h1);
  \draw [classical] (l2) -- (y1);

  \draw [classical] (lnce) -- (hn);
\end{tikzpicture}
\caption{Stacked autoencoders with euclidian distance as reconstruction loss and negative log-likelihood penalty to enforce sparsity of the extracted features.}
\label{fig:mlae}
\end{figure}
