Automatically generated by Mendeley Desktop 1.17.7
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/eugene/Downloads/!articles/!important/long{\_}shelhamer{\_}fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June-2015},
year = {2015}
}
@article{Zeiler2010,
abstract = {Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.},
archivePrefix = {arXiv},
arxivId = {1302.1700},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
doi = {10.1109/CVPR.2010.5539957},
eprint = {1302.1700},
file = {:home/eugene/Downloads/!articles/!important/cvpr2010.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2528--2535},
pmid = {16190471},
title = {{Deconvolutional networks}},
year = {2010}
}
@article{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02025v1},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
doi = {10.1038/nbt.3343},
eprint = {arXiv:1506.02025v1},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:pdf},
isbn = {9781627480031},
issn = {1087-0156},
journal = {Nips},
pages = {1--14},
pmid = {26571099},
title = {{Spatial Transformer Networks}},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/eugene/Downloads/!articles/!important/JMLRdropout.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@incollection{NIPS2013_5021,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and Weinberger, K Q},
file = {:home/eugene/Downloads/!articles/!important/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:pdf},
pages = {3111--3119},
publisher = {Curran Associates, Inc.},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@article{Beaver2012a,
abstract = {Abstract: We develop stochastic variational inference , a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and ...},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Beaver and Clark},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Beaver, Clark - 2012 - Stochastic Variational inference(2).pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
number = {2},
pages = {1303--1347},
pmid = {19926898},
title = {{Stochastic Variational inference}},
url = {http://arxiv.org/abs/1206.7051{\%}5Cnpapers2://publication/uuid/D5737928-F59F-43E3-8ADE-53F1831AA866},
volume = {14},
year = {2012}
}
@inproceedings{pennington2014glove,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://www.aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Glorot2011,
abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classi cation an interesting topic in academic and industrial research. Reviews can span so many di erent domains that it is dicult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classi ers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classi ers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
file = {:home/eugene/Downloads/!articles/!important/glorot2011domain.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning},
number = {1},
pages = {513--520},
title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
url = {http://www.icml-2011.org/papers/342{\_}icmlpaper.pdf},
year = {2011}
}
@inproceedings{bobic2012improving,
author = {Bobi{\'{c}}, Tamara and Klinger, Roman and Thomas, Philippe and Hofmann-Apitius, Martin},
booktitle = {Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP},
organization = {Association for Computational Linguistics},
pages = {35--43},
title = {{Improving distantly supervised extraction of drug-drug and protein-protein interactions}},
year = {2012}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:home/eugene/Downloads/!articles/!important/Yosinski{\_}{\_}2015{\_}{\_}ICML{\_}DL{\_}{\_}Understanding{\_}Neural{\_}Networks{\_}Through{\_}Deep{\_}Visualization{\_}{\_}.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Ng2011,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2011 - Sparse autoencoder.pdf:pdf},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lecture notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{DBLP:journals/corr/MiwaB16,
author = {Miwa, Makoto and Bansal, Mohit},
journal = {CoRR},
title = {{End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures}},
url = {http://arxiv.org/abs/1601.00770},
volume = {abs/1601.0},
year = {2016}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/eugene/Downloads/!articles/!important/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Lei2016,
abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
archivePrefix = {arXiv},
arxivId = {1606.04155},
author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1606.04155},
file = {:home/eugene/Downloads/!articles/!important/Rationalizing Neural Predictions (1).pdf:pdf},
journal = {EMNLP 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
pages = {107--117},
title = {{Rationalizing Neural Predictions}},
url = {http://arxiv.org/abs/1606.04155},
year = {2016}
}
@article{Dahl2017,
abstract = {We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.},
archivePrefix = {arXiv},
arxivId = {1702.00783},
author = {Dahl, Ryan and Norouzi, Mohammad and Shlens, Jonathon},
eprint = {1702.00783},
file = {:home/eugene/Downloads/!articles/!important/1702.00783.pdf:pdf},
title = {{Pixel Recursive Super Resolution}},
url = {http://arxiv.org/abs/1702.00783},
year = {2017}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {1506.02216},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@article{Mnih,
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Silver, David and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/eugene/Downloads/!articles/!important/dqn.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Dqn}}
}
@article{Warde-Farley2013,
abstract = {The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6197v2},
author = {Warde-Farley, D and Goodfellow, Ij},
eprint = {arXiv:1312.6197v2},
file = {:home/eugene/Downloads/!articles/!important/1312.6197.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
title = {{An empirical analysis of dropout in piecewise linear networks}},
url = {http://arxiv.org/pdf/1312.6197v2.pdf{\%}5Cnhttp://arxiv.org/abs/1312.6197},
year = {2013}
}
@article{Lever2009,
abstract = {"Boundary vector cells" were predicted to exist by computational models of the environmental inputs underlying the spatial firing patterns of hippocampal place cells (O'Keefe and Burgess, 1996; Burgess et al., 2000; Hartley et al., 2000). Here, we report the existence of cells fulfilling this description in recordings from the subiculum of freely moving rats. These cells may contribute environmental information to place cell firing, complementing path integrative information. Their relationship to other cell types, including medial entorhinal "border cells," is discussed.},
author = {Lever, Colin and Burton, Stephen and Jeewajee, Ali and O'Keefe, John and Burgess, Neil},
doi = {10.1523/JNEUROSCI.1319-09.2009},
file = {:home/eugene/Downloads/!articles/!important/Lever09{\_}BVCs.pdf:pdf},
isbn = {1529-2401 (Electronic) 0270-6474 (Linking)},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
number = {31},
pages = {9771--9777},
pmid = {19657030},
title = {{Boundary vector cells in the subiculum of the hippocampal formation.}},
volume = {29},
year = {2009}
}
@article{lecun-mnisthandwrittendigit-2010,
author = {LeCun, Yann and Cortes, Corinna},
howpublished = {http://yann.lecun.com/exdb/mnist/},
title = {{{\{}MNIST{\}} handwritten digit database}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {2010}
}
@article{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {:home/eugene/Downloads/!articles/!important/osdi16-abadi.pdf:pdf},
isbn = {978-1-931971-33-1},
journal = {Google Brain},
pages = {18},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{McMahan2013,
abstract = {Predicting ad click--through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v2},
author = {McMahan, H Brendan and Holt, Gary and Sculley, D and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
doi = {10.1145/2487575.2488200},
eprint = {arXiv:1301.3781v2},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McMahan et al. - 2013 - Ad click prediction a view from the trenches.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
journal = {Kdd},
keywords = {data mining,large-scale learning,online advertising},
pages = {1222--1230},
title = {{Ad click prediction: a view from the trenches}},
year = {2013}
}
@article{Gao2015a,
abstract = {This paper is concerned of the loop closure detection problem for visual simultaneous localization and mapping systems. We propose a novel approach based on the stacked denoising auto-encoder (SDA), a multi-layer neural network that autonomously learns an compressed representa-tion from the raw input data in an unsupervised way. Different with the traditional bag-of-words based methods, the deep network has the ability to learn the complex inner structures in image data, while no longer needs to manually design the visual features. Our approach employs the characteristics of the SDA to solve the loop detection problem. The workflow of training the network, utilizing the features and computing the similarity score is presented. The performance of SDA is evaluated by a comparison study with Fab-map 2.0 using data from open datasets and physical robots. The results show that SDA is feasible for detecting loops at a satisfactory pre-cision and can therefore provide an alternative way for visual SLAM systems.},
author = {Gao, Xiang and Zhang, Tao},
doi = {10.1007/s10514-015-9516-2},
file = {:home/eugene/Downloads/!articles/!important/art{\%}3A10.1007{\%}2Fs10514-015-9516-2.pdf:pdf},
isbn = {978-9-8815-6389-7},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Deep neural network,Loop closure detection,Simultaneous localization and mapping (SLAM),Stacked denoising auto-encoder},
number = {1},
pages = {1--18},
publisher = {Springer US},
title = {{Unsupervised learning to detect loops using deep neural networks for visual SLAM system}},
volume = {41},
year = {2015}
}
@article{Bengio2009a,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:home/eugene/Downloads/!articles/!important/ftml.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Nickolls2008,
abstract = {The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. Furthermore, their parallelism continues to scale with Moore's law. The challenge is to develop mainstream application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.},
author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
doi = {10.1145/1365490.1365500},
file = {:home/eugene/Downloads/!articles/!important/p40-nickolls.pdf:pdf},
isbn = {9783540773047},
issn = {15427730},
journal = {AMC Queue},
number = {April},
pages = {40--53},
title = {{Scalable parallel programming with CUDA}},
url = {www.acmqueue.com},
volume = {6},
year = {2008}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:home/eugene/Downloads/!articles/!important/1311.2901.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {https://arxiv.org/abs/1311.2901},
volume = {8689},
year = {2014}
}
@article{Kim2014,
abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vec- tors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5882v1},
author = {Kim, Yoon},
doi = {10.1109/LSP.2014.2325781},
eprint = {arXiv:1408.5882v1},
file = {:home/eugene/Downloads/!articles/!important/1408.5882.pdf:pdf},
isbn = {9781937284961},
issn = {10709908},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
pages = {1746--1751},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf},
year = {2014}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@incollection{Nguyen2015,
author = {Nguyen, Thien Huu and Grishman, Ralph},
booktitle = {Workshop on Vector Space Modeling for NLP at NAACL 2015},
title = {{Relation extraction: Perspective from convolutional neural networks}},
year = {2015}
}
@article{Karpathy2014,
abstract = {We present a model that generates free-form natural language descriptions of image regions. Our model leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between text and visual data. Our approach is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate the effectiveness of our alignment model with ranking experiments on Flickr8K, Flickr30K and MSCOCO datasets, where we substantially improve on the state of the art. We then show that the sentences created by our generative model outperform retrieval baselines on the three aforementioned datasets and a new dataset of region-level annotations.},
archivePrefix = {arXiv},
arxivId = {1412.2306v1},
author = {Karpathy, Andrej},
eprint = {1412.2306v1},
file = {:home/eugene/Downloads/!articles/!important/Karpathy{\_}Deep{\_}Visual-Semantic{\_}Alignments{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
journal = {arXiv},
title = {{Deep Visual Semantic Alignments for Generating Image Descriptions}},
year = {2014}
}
@article{Scherer2010,
abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57{\%} on the NORB normalized-uniform dataset and 5.6{\%} on the NORB jittered-cluttered dataset.},
author = {Scherer, Dominik and Muller, Andreas and Behnke, Sven},
doi = {10.1007/978-3-642-15825-4_10},
file = {:home/eugene/Downloads/!articles/!important/icann2010{\_}maxpool.pdf:pdf},
isbn = {3642158242},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {92--101},
title = {{Evaluation of pooling operations in convolutional architectures for object recognition}},
volume = {6354 LNCS},
year = {2010}
}
@article{Hartley2014,
abstract = {Over the past four decades, research has revealed that cells in the hippocampal formation provide an exquisitely detailed representation of an animal's current location and heading. These findings have provided the foundations for a growing understanding of the mechanisms of spatial cognition in mammals, including humans. We describe the key properties of the major categories of spatial cells: place cells, head direction cells, grid cells and boundary cells, each of which has a characteristic firing pattern that encodes spatial parameters relating to the animal's current position and orientation. These properties also include the theta oscillation, which appears to play a functional role in the representation and processing of spatial information. Reviewing recent work, we identify some themes of current research and introduce approaches to computational modelling that have helped to bridge the different levels of description at which these mechanisms have been investigated. These range from the level of molecular biology and genetics to the behaviour and brain activity of entire organisms. We argue that the neuroscience of spatial cognition is emerging as an exceptionally integrative field which provides an ideal test-bed for theories linking neural coding, learning, memory and cognition.},
author = {Hartley, Tom and Lever, Colin and Burgess, Neil and O'Keefe, John},
doi = {10.1098/rstb.2012.0510},
file = {:home/eugene/Downloads/!articles/!important/Hartley2014{\_}review.pdf:pdf},
isbn = {1471-2970 (Electronic)$\backslash$r0962-8436 (Linking)},
issn = {1471-2970},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {behaviour,cognition,neuroscience},
number = {1635},
pages = {20120510},
pmid = {24366125},
title = {{Space in the brain: how the hippocampal formation supports spatial cognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24366125},
volume = {369},
year = {2014}
}
@article{Salakhutdinov2009,
abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer " pre-training " phase that allows variational inference to be initialized with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and per-form well on handwritten digit and visual object recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1203.4416},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
doi = {10.1109/CVPRW.2009.5206577},
eprint = {1203.4416},
file = {:home/eugene/Downloads/!articles/!important/salakhutdinov09a.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {Aistats},
number = {3},
pages = {448--455},
title = {{Deep Boltzmann Machines}},
volume = {1},
year = {2009}
}
@article{jia2014caffe,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
journal = {arXiv preprint arXiv:1408.5093},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Boscaini2016,
abstract = {Establishing correspondence between shapes is a fundamental problem in geometry processing, arising in a wide variety of applications. The problem is especially difficult in the setting of non-isometric deformations, as well as in the presence of topological noise and missing parts, mainly due to the limited capability to model such deformations axiomatically. Several recent works showed that invariance to complex shape transformations can be learned from examples. In this paper, we introduce an intrinsic convolutional neural network architecture based on anisotropic diffusion kernels, which we term Anisotropic Convolutional Neural Network (ACNN). In our construction, we generalize convolutions to non-Euclidean domains by constructing a set of oriented anisotropic diffusion kernels, creating in this way a local intrinsic polar representation of the data (`patch'), which is then correlated with a filter. Several cascades of such filters, linear, and non-linear operators are stacked to form a deep neural network whose parameters are learned by minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.},
archivePrefix = {arXiv},
arxivId = {1605.06437},
author = {Boscaini, Davide and Masci, Jonathan and Rodol{\`{a}}, Emanuele and Bronstein, Michael M.},
eprint = {1605.06437},
file = {:home/eugene/Downloads/!articles/!important/1605.06437.pdf:pdf},
pages = {1--13},
title = {{Learning shape correspondence with anisotropic convolutional neural networks}},
url = {http://arxiv.org/abs/1605.06437},
year = {2016}
}
@article{tfd,
author = {{Susskind Joshua}, Anderson Adam and Hinton, Geof- frey E},
journal = {Technical report, De- partment of Computer Science, University of Toronto.},
title = {{The toronto face dataset.}},
year = {2010}
}
@article{Li2015,
abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02761v1},
author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
eprint = {arXiv:1502.02761v1},
file = {:home/eugene/Downloads/!articles/!important/li15.pdf:pdf},
isbn = {9781510810587},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1718--1727},
title = {{Generative Moment Matching Networks}},
url = {https://arxiv.org/abs/1502.02761},
volume = {37},
year = {2015}
}
@article{Carlsson2008,
author = {Carlsson, Gunnar and Ishkhanov, Tigran and de Silva, Vin and Zomorodian, Afra},
doi = {10.1007/s11263-007-0056-x},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlsson et al. - 2008 - On the local bahavior of spaces of natural images.pdf:pdf},
issn = {0920-5691},
journal = {Int. J. Comput. Vision},
keywords = {filtration,klein bottle,manifold,natural images,persistent homology,topology},
pages = {1--12},
title = {{On the local bahavior of spaces of natural images}},
volume = {76},
year = {2008}
}
@article{Louizos2016,
abstract = {We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational auto-encoding architecture with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the ``Maximum Mean Discrepancy'' (MMD) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations.},
archivePrefix = {arXiv},
arxivId = {1511.00830},
author = {Louizos, Christos and Swersky, Kevin and Li, Yujia and Welling, Max and Zemel, Richard},
eprint = {1511.00830},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louizos et al. - 2016 - The Variational Fair Auto Encoder.pdf:pdf},
journal = {Iclr},
pages = {1--11},
title = {{The Variational Fair Auto Encoder}},
url = {http://arxiv.org/abs/1511.00830},
year = {2016}
}
@article{Zhou2016,
abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.},
archivePrefix = {arXiv},
arxivId = {1610.02055},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Torralba, Antonio and Oliva, Aude},
eprint = {1610.02055},
file = {:home/eugene/Downloads/!articles/!important/places2{\_}arxiv.pdf:pdf},
journal = {ArXiv},
pages = {1--12},
title = {{Places: An Image Database for Deep Scene Understanding}},
url = {http://arxiv.org/abs/1610.02055},
year = {2016}
}
@article{Mishkin2015,
abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple strategy for weight initialization for deep net learning - is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06422},
author = {Mishkin, Dmytro and Matas, Jiri},
doi = {10.1016/0898-1221(96)87329-9},
eprint = {1511.06422},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishkin, Matas - 2015 - All you need is a good init.pdf:pdf},
isbn = {0262560992},
issn = {08981221},
journal = {Iclr},
keywords = {Initialization,Optimization},
pages = {1--8},
pmid = {21595383},
title = {{All you need is a good init}},
url = {http://arxiv.org/abs/1511.06422},
year = {2015}
}
@article{Rifai2011,
abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rifai, Salah and Muller, Xavier},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/eugene/Downloads/!articles/!important/ICML2011Rifai{\_}455.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {1467-9280},
journal = {Icml},
number = {1},
pages = {833--840},
pmid = {25052830},
title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
volume = {85},
year = {2011}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
eprint = {1606.05908},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
journal = {arXiv},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--23},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}
@article{DBLP:journals/corr/TaiSM15,
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
journal = {CoRR},
title = {{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}},
url = {http://arxiv.org/abs/1503.00075},
volume = {abs/1503.0},
year = {2015}
}
@article{Lample2016,
abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.},
archivePrefix = {arXiv},
arxivId = {1609.05521},
author = {Lample, Guillaume and Chaplot, Devendra Singh},
eprint = {1609.05521},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lample, Chaplot - 2016 - Playing FPS Games with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv cs.AI},
number = {2015},
pages = {05521},
title = {{Playing FPS Games with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05521},
volume = {9},
year = {2016}
}
@article{Patraucean2016,
abstract = {We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort.},
archivePrefix = {arXiv},
arxivId = {1511.06309},
author = {Patraucean, Viorica and Handa, Ankur and Cipolla, Roberto},
eprint = {1511.06309},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patraucean, Handa, Cipolla - 2016 - Spatio-temporal video autoencoder with differentiable memory.pdf:pdf},
journal = {International Conference On Learning Representations},
number = {2015},
pages = {1--10},
title = {{Spatio-temporal video autoencoder with differentiable memory}},
url = {http://arxiv.org/abs/1511.06309},
year = {2016}
}
@article{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever et al. - 2013 - On the importance of initialization and momentum in deep learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
journal = {Jmlr W{\&}Cp},
keywords = {dblp},
number = {2010},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://dblp.uni-trier.de/db/conf/icml/icml2013.html{\#}SutskeverMDH13},
volume = {28},
year = {2013}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/eugene/Downloads/!articles/!important/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@book{Good2016,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Iqbal2017,
abstract = {Action recognition is a fundamental problem in computer vision with alot of poten- tial applications such as video surveillance, human computer interaction and robot learning. Given pre-segmented videos, the task is to recognize actions happening within videos. Historically, hand crafted video features were used to address the task of action recognition. With the success of Deep ConvNets as image analysis method, a lot of extensions of standard ConvNets were purposed to process variable length video data. In this work, we propose novel recurrent ConvNet architecture called Spatio Temporal Residual networks to address the task of action recognition. The approach extends state of the art image analysis model called ResNet [He {\&} Zhang + 15]. We show that on large-scale dataset, our model improves over the standard ResNet [He {\&} Zhang + 15] architecture},
author = {Iqbal, Ahsan},
file = {:home/eugene/Downloads/!articles/!important/main.pdf:pdf},
isbn = {4962218891},
number = {January},
pages = {110608},
title = {{Convolutional Networks for Action Recognition}},
year = {2017}
}
@inproceedings{hendrickx2009semeval,
author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and {{\'{O}} S{\'{e}}aghdha}, Diarmuid and Pad{\'{o}}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
booktitle = {Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions},
organization = {Association for Computational Linguistics},
pages = {94--99},
title = {{Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals}},
year = {2009}
}
@article{Cauchy1847,
author = {Cauchy, Augustine-Louis},
file = {:home/eugene/Downloads/!articles/!important/cauchy-en.pdf:pdf},
journal = {Compte Rendu des Seances de L'Acad'emie des Sciences},
number = {2},
pages = {536--538},
title = {{Methode generale pour la resolution des systemes d'equations simultanees}},
volume = {25},
year = {1847}
}
@article{Clevert2015,
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers ({\textgreater}4). ELU networks were among top 10 reported CIFAR-10 results and yielded the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELUs considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork-Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
eprint = {1511.07289},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clevert, Unterthiner, Hochreiter - 2015 - Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).pdf:pdf},
journal = {Under review of ICLR2016， 提出了ELU},
number = {1997},
pages = {1--13},
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}},
url = {http://arxiv.org/pdf/1511.07289.pdf{\%}5Cnhttp://arxiv.org/abs/1511.07289},
year = {2015}
}
@inproceedings{Mintz:2009:DSR:1690219.1690287,
address = {Stroudsburg, PA, USA},
author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2},
isbn = {978-1-932432-46-6},
pages = {1003--1011},
publisher = {Association for Computational Linguistics},
series = {ACL '09},
title = {{Distant Supervision for Relation Extraction Without Labeled Data}},
url = {http://dl.acm.org/citation.cfm?id=1690219.1690287},
year = {2009}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Srivastava2015,
abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1505.00387},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
eprint = {1505.00387},
file = {:home/eugene/Downloads/!articles/!important/1505.00387.pdf:pdf},
journal = {arXiv:1505.00387 [cs]},
title = {{Highway Networks}},
url = {http://arxiv.org/abs/1505.00387},
year = {2015}
}
@inproceedings{culotta2006integrating,
author = {Culotta, Aron and McCallum, Andrew and Betz, Jonathan},
booktitle = {Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
organization = {Association for Computational Linguistics},
pages = {296--303},
title = {{Integrating probabilistic extraction models and data mining to discover relations and patterns in text}},
year = {2006}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/pdf/1512.03385v1.pdf},
volume = {7},
year = {2015}
}
@article{Dollar2009,
abstract = {Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field.},
author = {Doll{\'{a}}r, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
doi = {10.1109/CVPRW.2009.5206631},
file = {:home/eugene/Downloads/!articles/!important/05206631.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {304--311},
title = {{Pedestrian detection: A benchmark}},
year = {2009}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam a Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/eugene/Downloads/!articles/!important/1312.5602.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {arXiv:1402.1869v1},
file = {:home/eugene/Downloads/!articles/!important/1402.1869.pdf:pdf},
issn = {10495258},
journal = {Nips},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {1--12},
title = {{On the Number of Linear Regions of Deep Neural Networks}},
year = {2014}
}
@article{Stollenga2015,
abstract = {Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the en- tire spatio-temporal context of each pixel in a few sweeps through all pixels, es- pecially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).},
archivePrefix = {arXiv},
arxivId = {1506.0745},
author = {Stollenga, MF and Byeon, W},
eprint = {1506.0745},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stollenga, Byeon - 2015 - M Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation.pdf:pdf},
issn = {10495258},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {7},
title = {{[M] Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation}},
url = {http://arxiv.org/abs/1506.0745{\%}5Cnhttp://arxiv.org/abs/1506.07452},
volume = {di},
year = {2015}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatys, Ecker, Bethge - 2015 - A Neural Algorithm of Artistic Style.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {eural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/eugene/Downloads/!articles/!important/AISTATS2010{\_}GlorotB10.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf},
volume = {9},
year = {2010}
}
@article{Larsen2015,
abstract = {We present an autoencoder that leverages the power of learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors that better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that our method outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that our method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
archivePrefix = {arXiv},
arxivId = {1512.09300},
author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
eprint = {1512.09300},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsen et al. - 2015 - Autoencoding beyond pixels using a learned similarity metric.pdf:pdf},
isbn = {9781510829008},
journal = {arXiv preprint arxiv:1512.09300},
title = {{Autoencoding beyond pixels using a learned similarity metric}},
url = {http://arxiv.org/abs/1512.09300},
year = {2015}
}
@article{Huang2016,
author = {Huang, Yao and Sun, Fuchun and Guo, Yao},
file = {:home/eugene/Downloads/!articles/!important/07831876.pdf:pdf},
isbn = {9781509041022},
number = {August},
pages = {511--516},
title = {{VLAD-Based Loop Closure Detection For Monocular SLAM}},
year = {2016}
}
@article{Xia2016,
abstract = {Loop closure detection benefits simultaneous localization and mapping (SLAM) in building a consistent map of the environment by reducing the accumulate error. Handcrafted features have been successfully used in traditional approaches, whereas in this paper, we show that unsupervised features extracted by deep learning models, can improves the accuracy of loop closure detection. In particular, we employ a cascaded deep network, namely the PCANet, to extract features as image descriptors. We tested the performance of our proposed method on open datasets to compare with traditional approaches. We found that the PCANet features outperform state-of-the-art handcrafted competitors, and are computational efficient to be implemented in practical robotics.},
author = {Xia, Yifan and Li, Jie and Qi, Lin and Fan, Hao},
doi = {10.1109/IJCNN.2016.7727481},
file = {:home/eugene/Downloads/!articles/!important/07727481.pdf:pdf},
isbn = {978-1-5090-0620-5},
journal = {2016 International Joint Conference on Neural Networks (IJCNN)},
keywords = {Loop Closure Detection,PCANet,Simultaneous Localization and Mapping},
pages = {2274--2281},
title = {{Loop closure detection for visual SLAM using PCANet features}},
url = {http://ieeexplore.ieee.org/abstract/document/7727481},
year = {2016}
}
@article{Luo05022016,
author = {Luo, Yuan and Uzuner, {\"{O}}zlem and Szolovits, Peter},
doi = {10.1093/bib/bbw001},
journal = {Briefings in Bioinformatics},
title = {{Bridging semantics and syntax with graph algorithms—state-of-the-art of extracting biomedical relations}},
url = {http://bib.oxfordjournals.org/content/early/2016/02/04/bib.bbw001.abstract},
year = {2016}
}
@article{Arora2014,
abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an {\$}n{\$} node multilayer neural net that has degree at most {\$}n{\^{}}{\{}\backslashgamma{\}}{\$} for some {\$}\backslashgamma {\textless}1{\$} and each edge has a random edge weight in {\$}[-1,1]{\$}. Our algorithm learns {\{}$\backslash$em almost all{\}} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
archivePrefix = {arXiv},
arxivId = {1310.6343},
author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
eprint = {1310.6343},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora et al. - 2014 - Provable Bounds for Learning Some Deep Representations.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning},
pages = {18},
title = {{Provable Bounds for Learning Some Deep Representations}},
url = {http://arxiv.org/abs/1310.6343},
volume = {32},
year = {2014}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:home/eugene/Downloads/!articles/!important/icml2010{\_}NairH10.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Doll,
author = {Doll, Piotr and Wojek, Christian and Perona, Pietro},
doi = {10.1109/CVPRW.2009.5206631},
file = {:home/eugene/Downloads/!articles/!important/DollarCVPR09peds.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
title = {{Pedestrian Detection : A Benchmark}}
}
@article{Makhzani2015,
abstract = {In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named "adversarial autoencoder", uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no "holes" in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian},
eprint = {1511.05644},
file = {:home/eugene/Downloads/!articles/!important/1511.05644v2.pdf:pdf},
isbn = {9781509008063},
journal = {arXiv},
pages = {1--10},
title = {{Adversarial Autoencoders}},
url = {http://arxiv.org/abs/1511.05644},
year = {2015}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/eugene/Downloads/!articles/!important/1412.6572.pdf:pdf},
isbn = {1412.6572},
journal = {Iclr 2015},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2015}
}
@article{Bauckhage2015,
author = {Bauckhage, Christian},
doi = {10.13140/2.1.4453.2009},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bauckhage - 2015 - NumPy SciPy Recipes for Data Science k -Medoids Clustering.pdf:pdf},
number = {April},
pages = {1--6},
title = {{NumPy / SciPy Recipes for Data Science : k -Medoids Clustering}},
volume = {1},
year = {2015}
}
@software{Slade3,
author = {Judd, Simon and Lysiuk, Alexey},
howpublished = {N64-Spielmodul},
publisher = {Nintendo},
title = {{SLADE3}},
url = {http://slade.mancubus.net},
year = {2016}
}
@article{Long2014,
abstract = {Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1091v1},
author = {Long, Jonathan and Zhang, Ning and Darrell, Trevor},
doi = {10.1007/978-3-642-33863-2_51},
eprint = {arXiv:1411.1091v1},
file = {:home/eugene/Downloads/!articles/!important/5420-do-convnets-learn-correspondence.pdf:pdf},
isbn = {9783642338625},
issn = {03029743},
journal = {Advances in Neural Information {\ldots}},
pages = {1601--1609},
pmid = {16237996},
title = {{Do Convnets Learn Correspondence?}},
url = {http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf{\%}5Cnhttp://papers.nips.cc/paper/5420-do-convnets-learn-correspondence},
year = {2014}
}
@article{Kalchbrenner2016,
abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32{\%} on MNIST.},
archivePrefix = {arXiv},
arxivId = {1507.01526},
author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
doi = {10.1016/j.physe.2015.09.035},
eprint = {1507.01526},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalchbrenner, Danihelka, Graves - 2016 - Grid Long Short-Term Memory.pdf:pdf},
journal = {Iclr},
pages = {14},
title = {{Grid Long Short-Term Memory}},
url = {http://arxiv.org/abs/1507.01526},
year = {2016}
}
@article{Nakov2016,
abstract = {This paper discusses the fourth year of the " Sentiment Analysis in Twitter Task " . SemEval-2016 Task 4 comprises five sub-tasks, three of which represent a significant departure from previous editions. The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet. The three new subtasks focus on two variants of the basic " sentiment classification in Twitter " task. The first variant adopts a five-point scale, which confers an ordinal character to the clas-sification task. The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learn-ing literature. The task continues to be very popular, attracting a total of 43 teams.},
author = {Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Stoyanov, Veselin and Sebastiani, Fabrizio},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nakov et al. - 2016 - {\{}SemEval{\}}-2016 Task 4 Sentiment Analysis in {\{}T{\}}witter.pdf:pdf},
journal = {Proceedings of the 10th International Workshop on Semantic Evaluation},
pages = {1--18},
title = {{{\{}SemEval{\}}-2016 Task 4: Sentiment Analysis in {\{}T{\}}witter}},
year = {2016}
}
@article{Moser2008,
abstract = {More than three decades of research have demonstrated a role for hippocampal place cells in representation of the spatial environment in the brain. New studies have shown that place cells are part of a broader circuit for dynamic representation of self-location. A key component of this network is the entorhinal grid cells, which, by virtue of their tessellating firing fields, may provide the elements of a path integration-based neural map. Here we review how place cells and grid cells may form the basis for quantitative spatiotemporal representation of places, routes, and associated experiences during behavior and in memory. Because these cell types have some of the most conspicuous behavioral correlates among neurons in nonsensory cortical systems, and because their spatial firing structure reflects computations internally in the system, studies of entorhinal-hippocampal representations may offer considerable insight into general principles of cortical network dynamics.},
author = {Moser, Edvard I and Kropff, Emilio and Moser, May-Britt},
doi = {10.1146/annurev.neuro.31.061307.090723},
file = {:home/eugene/Downloads/!articles/!important/annurev.neuro.31.061307.pdf:pdf},
isbn = {0147-006X (Print)$\backslash$n0147-006X (Linking)},
issn = {0147-006X},
journal = {Annual review of neuroscience},
keywords = {attractor,entorhinal cortex,hippocampus,memory,path integration},
pages = {69--89},
pmid = {18284371},
title = {{Place cells, grid cells, and the brain's spatial representation system.}},
volume = {31},
year = {2008}
}
@article{Mountney2006,
abstract = {Minimally Invasive Surgery (MIS) has recognized benefits of reduced patient trauma and recovery time. In practice, MIS procedures present a number of challenges due to the loss of 3D vision and the narrow field-of-view provided by the camera. The restricted vision can make navigation and localization within the human body a challenging task. This paper presents a robust technique for building a repeatable long term 3D map of the scene whilst recovering the camera movement based on Simultaneous Localization and Mapping (SLAM). A sequential vision only approach is adopted which provides 6 DOF camera movement that exploits the available textured surfaces and reduces reliance on strong planar structures required for range finders. The method has been validated with a simulated data set using real MIS textures, as well as in vivo MIS video sequences. The results indicate the strength of the proposed algorithm under the complex reflectance properties of the scene, and the potential for real-time application for integrating with the existing MIS hardware.},
author = {Mountney, Peter and Stoyanov, Danail and Davison, Andrew and Yang, Guang-Zhong},
doi = {10.1007/11866565_43},
file = {:home/eugene/Downloads/!articles/!important/mountney{\_}etal{\_}miccai2006 (1).pdf:pdf},
isbn = {978-3-540-44707-8},
issn = {0302-9743},
journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
keywords = {Algorithms,Biological,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Connective Tissue,Connective Tissue: anatomy {\&} histology,Connective Tissue: surgery,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Laparoscopy,Laparoscopy: methods,Minimally Invasive,Minimally Invasive: methods,Models,Reproducibility of Results,Sensitivity and Specificity,Surgery,Surgical Procedures,Three-Dimensional,Three-Dimensional: methods,User-Computer Interface},
number = {Pt 1},
pages = {347--354},
pmid = {17354909},
title = {{Simultaneous stereoscope localization and soft-tissue mapping for minimal invasive surgery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17354909},
volume = {9},
year = {2006}
}
@article{Kempka2016,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Kempka, Micha{\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\'{s}}kowski, Wojciech},
eprint = {1605.02097},
file = {:home/eugene/Downloads/!articles/!important/1605.02097.pdf:pdf},
journal = {arXiv:1605.02097v1 [cs.LG]},
keywords = {deep reinforcement learning,first-person perspective games,fps,neural networks,video games,visual learning,visual-based reinforcement learning},
title = {{ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning}},
url = {http://arxiv.org/abs/1605.02097},
year = {2016}
}
@article{RanzatoMarcAurelio2007,
abstract = {We present an unsupervised method for learning a hi-erarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extrac-tor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each fil-ter output within adjacent windows, and a point-wise sig-moid non-linearity. A second level of larger and more in-variant features is obtained by training the same algorithm on patches of features from the first level. Training a su-pervised classifier on these features yields 0.64{\%} error on MNIST, and 54{\%} average recognition rate on Caltech 101 with 30 training samples per category. While the result-ing architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely super-vised learning procedures, and yields good performance with very few labeled training samples.},
author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann},
file = {:home/eugene/Downloads/!articles/!important/ranzato-cvpr-07.pdf:pdf},
isbn = {1424411807},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1--8},
title = {{Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/ranzato-cvpr-07.pdf},
year = {2007}
}
@article{Ren2016,
abstract = {Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment- matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.},
archivePrefix = {arXiv},
arxivId = {1606.04218},
author = {Ren, Yong and Li, Jialian and Luo, Yucen and Zhu, Jun},
eprint = {1606.04218},
file = {:home/eugene/Downloads/!articles/!important/1606.04218.pdf:pdf},
journal = {Nips},
number = {2},
pages = {1--12},
title = {{Conditional Generative Moment-Matching Networks}},
url = {http://arxiv.org/abs/1606.04218},
year = {2016}
}
@article{Johnson2016,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
archivePrefix = {arXiv},
arxivId = {1511.07571},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
doi = {10.1109/CVPR.2016.494},
eprint = {1511.07571},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Karpathy, Fei-Fei - 2016 - DenseCap Fully Convolutional Localization Networks for Dense Captioning.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
pages = {4565--4574},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://arxiv.org/abs/1511.07571},
year = {2016}
}
@misc{Rumelhart,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:home/eugene/Downloads/!articles/!important/backprop{\_}old.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
pmid = {134},
title = {{Learning representatons by back-propagating errors}}
}
@article{Kulkarni2015,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Td and Whitney, W},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
file = {:home/eugene/Downloads/!articles/!important/5851-deep-convolutional-inverse-graphics-network.pdf:pdf},
issn = {10897550},
journal = {Advances in Neural Information Processing Systems},
pages = {2539--2547},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:pdf},
isbn = {1212.5701},
journal = {arXiv},
pages = {6},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Conneau2016,
abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.},
archivePrefix = {arXiv},
arxivId = {1606.01781},
author = {Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"{i}}c and Lecun, Yann},
doi = {10.1007/s13218-012-0198-z},
eprint = {1606.01781},
file = {:home/eugene/Downloads/!articles/!important/Very Deep Convolutional Networks for NLP.pdf:pdf},
isbn = {9789814618038},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
number = {4},
pages = {357--363},
pmid = {10463930},
title = {{Very Deep Convolutional Networks for Natural Language Processing}},
url = {http://link.springer.com/10.1007/s13218-012-0198-z{\%}5Cnhttp://arxiv.org/abs/1606.01781},
volume = {26},
year = {2016}
}
@article{Wexler2004,
abstract = {We present a method for space-time completion of large space-time "holes" in video sequences of complex dynamic scenes. The missing portions are filled-in by sampling spatio-temporal patches from the available parts of the video, while enforcing global spatio-temporal consistency between all patches in and around the hole. This is obtained by posing the task of video completion and synthesis as a global optimization problem with a well-defined objective function. The consistent completion of static scene parts simultaneously with dynamic behaviors leads to realistic looking video sequences. Space-time video completion is useful for a variety of tasks, including, but not limited to: (i) Sophisticated video removal (of undesired static or dynamic objects) by completing the appropriate static or dynamic background information, (ii) Correction of missing/corrupted video frames in old movies, and (iii) Synthesis of new video frames to add a visual story, modify it, or generate a new one. Some examples of these are shown in the paper.},
author = {Wexler, Y. and Shechtman, E. and Irani, Michal},
doi = {10.1109/CVPR.2004.1315022},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wexler, Shechtman, Irani - 2004 - Space-time video completion.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {1063-6919},
journal = {Proc. of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {I--120-- I--127 Vol.1},
title = {{Space-time video completion}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1315022{\%}5Cnhttp://ieeexplore.ieee.org/ielx5/9183/29133/01315022.pdf?tp={\&}arnumber=1315022{\&}isnumber=29133},
volume = {1},
year = {2004}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Wu2015,
abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
archivePrefix = {arXiv},
arxivId = {1512.00242},
author = {Wu, Haibing and Gu, Xiaodong},
doi = {10.1016/j.neunet.2015.07.007},
eprint = {1512.00242},
file = {:home/eugene/Downloads/!articles/!important/1512.00242.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Convolutional neural networks,Deep learning,Max-pooling dropout},
pages = {1--10},
pmid = {26277608},
title = {{Towards dropout training for convolutional neural networks}},
volume = {71},
year = {2015}
}
@article{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:home/eugene/Downloads/!articles/!important/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{pubmed,
author = {{for Biotechnology Information}, National Center},
howpublished = {$\backslash$url{\{}https://www.ncbi.nlm.nih.gov/pubmed{\}}},
title = {{PubMed}}
}
@article{DBLP:journals/corr/ShazeerDEW16,
author = {Shazeer, Noam and Doherty, Ryan and Evans, Colin and Waterson, Chris},
journal = {CoRR},
title = {{Swivel: Improving Embeddings by Noticing What's Missing}},
url = {http://arxiv.org/abs/1602.02215},
volume = {abs/1602.0},
year = {2016}
}
@article{Choromanska2015,
abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
archivePrefix = {arXiv},
arxivId = {1412.0233},
author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'{e}}rard Ben and LeCun, Yann},
eprint = {1412.0233},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf:pdf},
isbn = {1412.0233},
issn = {15337928},
journal = {Aistats},
keywords = {Neural Network,Optimization},
pages = {192----204},
title = {{The Loss Surfaces of Multilayer Networks}},
url = {http://arxiv.org/abs/1412.0233{\%}5Cnhttp://www.arxiv.org/pdf/1412.0233.pdf},
volume = {38},
year = {2015}
}
@article{Taube1990a,
abstract = {This paper is a study of the behavioral and spatial firing correlates of neurons in the rat postsubiculum. Recordings were made from postsubicular neurons as rats moved freely throughout a cylindrical chamber, where the major cue for orientation was a white card taped to the inside wall. An automatic video/computer system monitored cell discharge while simultaneously tracking the position of 2 colored light emitting diodes (LEDs) secured to the animal's head. The animal's location was calculated from the position of one of the LEDs and head direction in the horizontal plane calculated from the relative positions of the 2 LEDs. Approximately 26{\%} of the cells were classified as head-direction cells because they discharged as a function of the animal's head direction in the horizontal plane, independent of the animal's behavior, location, or trunk position. For each head-direction cell, vectors drawn in the direction of maximal firing were parallel throughout the recording chamber and did not converge toward a single point. Plots of firing rate versus head direction showed that each firing-rate/head-direction function was adequately described by a triangular function. Each cell's maximum firing rate occurred at only one (the preferred) head direction; firing rates at head directions on either side of the preferred direction decreased linearly with angular deviation from the preferred direction. Results from 24 head-direction cells in 7 animals showed an equal distribution of preferred firing directions over a 360 degrees angle. The peak firing rate of head-direction cells varied from 5 to 115 spikes/sec (mean: 35). The range of head-direction angles over which discharge was elevated (directional firing range) was usually about 90 degrees, with little, if any, discharge at head directions outside this range. Quantitative analysis showed the location of the animal within the cylinder had minimal effect on directional cell firing. For each head-direction cell, the preferred direction, peak firing rate, and directional firing range remained stable for days. These results identify a new cell type that signals the animal's head direction in its environment.},
author = {Taube, J S and Muller, R U and Ranck, J B},
doi = {10.1212/01.wnl.0000299117.48935.2e},
file = {:home/eugene/Downloads/!articles/!important/420.full.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
number = {2},
pages = {420--35},
pmid = {2303851},
title = {{Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2303851},
volume = {10},
year = {1990}
}
@article{Dauphin14,
author = {Dauphin, Yann and Pascanu, Razvan and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
journal = {CoRR},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
volume = {abs/1406.2},
year = {2014}
}
@article{DBLP:journals/corr/WangJ16a,
author = {Wang, Shuohang and Jiang, Jing},
journal = {CoRR},
title = {{Machine Comprehension Using Match-LSTM and Answer Pointer}},
url = {http://arxiv.org/abs/1608.07905},
volume = {abs/1608.0},
year = {2016}
}
@article{Dosovitskiy2015a,
abstract = {Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features when combined with a strong prior. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.},
archivePrefix = {arXiv},
arxivId = {1506.02753},
author = {Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-10593-2_13},
eprint = {1506.02753},
file = {:home/eugene/Downloads/!articles/!important/1506.02753.pdf:pdf},
isbn = {9781941643327},
issn = {15505499},
journal = {arXiv preprint arXiv:1506.02753},
pages = {1--15},
title = {{Inverting Visual Representations with Convolutional Networks}},
url = {http://arxiv.org/abs/1506.02753},
year = {2015}
}
@article{Nguyen2015,
abstract = {Up to now, relation extraction systems have made extensive use of features generated by linguistic analysis modules. Errors in these features lead to errors of relation detection and classification. In this work, we depart from these traditional approaches with complicated feature engineering by introducing a convolu-tional neural network for relation extraction that automatically learns features from sen-tences and minimizes the dependence on ex-ternal toolkits and resources. Our model takes advantages of multiple window sizes for fil-ters and pre-trained word embeddings as an initializer on a non-static architecture to im-prove the performance. We emphasize the re-lation extraction problem with an unbalanced corpus. The experimental results show that our system significantly outperforms not only the best baseline systems for relation extrac-tion but also the state-of-the-art systems for relation classification.},
author = {Nguyen, Thien Huu and Grishman, Ralph},
doi = {10.3115/v1/W15-1506},
file = {:home/eugene/Downloads/!articles/!important/vector15.pdf:pdf},
journal = {Workshop on Vector Modeling for NLP},
pages = {39--48},
title = {{Relation Extraction: Perspective from Convolutional Neural Networks}},
year = {2015}
}
@article{Ackley1985,
abstract = {The computotionol elements connections fraction lem in o very networks but to use search there must preexisting straints method, eral knowledge examples thot tivity power resides between of the appear the technique knowledge short time. connections that be some hardware in the domain based learning ore rule obout in which demonstrobly structure.},
author = {Ackley and Hinton and Sejnowski and Ackley, D H and Hinton, G E and Sejnowski, T J},
doi = {10.1016/S0364-0213(85)80012-4},
file = {:home/eugene/Downloads/!articles/!important/Ackley{\_}et{\_}al-1985-Cognitive{\_}Science.pdf:pdf},
isbn = {0934613338},
issn = {03640213},
journal = {Cognitive Science},
pages = {147--169},
title = {{A learning Algorithm for Boltzmann Machines}},
url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
volume = {9},
year = {1985}
}
@article{Zikos2016,
abstract = {Environment perception is a crucial ability for robot's inter-action into an environment. One of the first steps in this direction is the combined problem of simultaneous localization and mapping (SLAM). A new method, called G-SLAM, is proposed, where the map is consid-ered as a set of scattered points in the continuous space followed by a probability that states the existence of an obstacle in the subsequent point in space. A probabilistic approach with particle filters for the ro-bot's pose estimation and an adaptive recursive algorithm for the map's probability distribution estimation is presented. Key feature of the G-SLAM method is the adaptive repositioning of the scattered points and their convergence around obstacles. In this paper the goal is to estimate the best robot trajectory along with the probability distribution of the obstacles in space. For experimental purposes a four wheel rear drive car kinematic model is used and results derived from real case scenarios are discussed.},
archivePrefix = {arXiv},
arxivId = {1607.05217},
author = {Zikos, Nikos and Petridis, Vassilios},
eprint = {1607.05217},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zikos, Petridis - Unknown - GENERATIVE SIMULTANEOUS LOCALIZATION AND MAPPING (G-SLAM).pdf:pdf},
pages = {1--14},
title = {{Generative Simultaneous Localization and Mapping (G-Slam)}},
url = {http://arxiv.org/abs/1607.05217},
year = {2016}
}
@article{Debao1993,
abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
author = {Cybenko, George},
doi = {10.1007/BF02836480},
file = {:home/eugene/Downloads/!articles/!important/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf:pdf},
isbn = {0780300564},
issn = {10009221},
journal = {Approximation Theory and its Applications},
keywords = {approximation,completeness,neural networks},
number = {3},
pages = {17--28},
title = {{Approximations by superpositions of sigmoidal functions}},
volume = {9},
year = {1989}
}
@article{Taube1990,
abstract = {The discharge characteristics of postsubicular head-direction cells in a fixed environment were described in the previous paper (Taube et al., 1990). This paper reports changes in the firing properties of head-direction cells following changes in the animal's environment. Head-direction cells were recorded from rats as they moved freely in a 76-cm-diameter gray cylinder. A white card, occupying 100 degrees of arc, was taped to the inside wall of the cylinder and served as the major orienting spatial cue in the animal's environment. Rotation of the cue card produced near-equal rotation in the preferred firing direction of head-direction cells, with minimal changes in peak firing rate, directional firing range, or asymmetry of the firing-rate/head-direction function. Card removal had no effect on peak firing rate or range of firing, but in 8/13 cells the preferred direction rotated by at least 24 degrees. Similarly, changing the shape of the environment to a rectangular or square enclosure caused the preferred firing direction to rotate by at least 48 degrees for 8/10 cells in the rectangle and 3/8 cells in the square, with minimal changes in the peak firing rate or directional firing range. Hand holding the animals and moving them around the cylinder had no effect on the preferred direction or firing range of the cell, but decreased the maximal firing rate in 7/9 cells. On 2 occasions, 2 head-direction cells were recorded simultaneously. The rotation of the preferred firing direction for one cell was the same as the rotation of the preferred direction for the second cell after each environmental manipulation. These results demonstrate that specific visual cues in the environment can exert control over the preferred firing direction and indicate that head-direction cell firing is not a simple sensory response to visual cues, but rather represents more abstract information concerning the animal's spatial relationship with its environment. The constancy of the angle between the preferred firing directions of pairs of simultaneously recorded head-direction cells suggests that there is a fixed mapping of the population onto direction within the environment. Thus, environmental manipulations appear to cause only a change in the reference direction, but leave all other discharge characteristics of directional cells unchanged. In the discussion, comparisons are drawn between the responses of head-direction cells and hippocampal place cells to similar environmental manipulations (Muller and Kubie, 1987), and ways in which these 2 spatial systems interact in navigation are discussed.(ABSTRACT TRUNCATED AT 400 WORDS)},
author = {Taube, JS S and Muller, RU U and {Ranck, JB}, Jr B},
doi = {10.1212/01.wnl.0000299117.48935.2e},
file = {:home/eugene/Downloads/!articles/!important/436.full.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474},
journal = {J. Neurosci.},
number = {2},
pages = {436--447},
pmid = {2303852},
title = {{Head-direction cells recorded from the postsubiculum in freely moving rats. II. Effects of environmental manipulations}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/2303852},
volume = {10},
year = {1990}
}
@article{Mathieu2015,
abstract = {Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction is viewed as a promising avenue for unsupervised feature learning. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.},
archivePrefix = {arXiv},
arxivId = {1511.05440},
author = {Mathieu, Michael and Couprie, Camille and LeCun, Yann},
eprint = {1511.05440},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mathieu, Couprie, LeCun - 2015 - Deep multi-scale video prediction beyond mean square error.pdf:pdf},
journal = {Iclr},
number = {2015},
pages = {1--14},
title = {{Deep multi-scale video prediction beyond mean square error}},
url = {http://arxiv.org/abs/1511.05440},
year = {2015}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55{\%} using no external data which is over a 20{\%} relative improvement on the previous year's winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Andrew G.},
eprint = {1312.5402},
file = {:home/eugene/Downloads/!articles/!important/1312.5402.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5402},
pages = {1--6},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Gao2015,
abstract = {The detection of loop closure is of essential importance in visual simultaneous localization and mapping systems. It can reduce the accumulating drift of localization algorithms if the loops are checked correctly. Traditional loop closure detection approaches take advantage of Bag-of-Words model, which clusters the feature descriptors as words and measures the similarity between the observations in the word space. However, the features are usually designed artificially and may not be suitable for data from new-coming sensors. In this paper a novel loop closure detection approach is proposed that learns features from raw data using deep neural networks instead of common visual features. We discuss the details of the method of training neural networks. Experiments on an open dataset are also demonstrated to evaluate the performance of the proposed method. It can be seen that the neural network is feasible to solve this problem.},
author = {Gao, Xiang and Zhang, Tao},
doi = {10.1109/ChiCC.2015.7260555},
file = {:home/eugene/Downloads/!articles/!important/07260555.pdf:pdf},
isbn = {978-9-8815-6389-7},
issn = {21612927},
journal = {2015 34th Chinese Control Conference (CCC)},
keywords = {Deep Neural Networks,Denoising Autoencoder,Feature extraction,Loop Closure Detection,Machine learning,Neural networks,SLAM (robots),Simultaneous Localization and Mapping,Simultaneous localization and mapping,Sparse matrices,Training,Visualization,bag-of-words,deep neural networks,loop closure detection,neurocontrollers,robot vision,visual SLAM systems,visual simultaneous localization and mapping syste},
number = {1},
pages = {5851--5856},
title = {{Loop closure detection for visual SLAM systems using deep neural networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7260555},
year = {2015}
}
@article{Zhao2015,
abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
archivePrefix = {arXiv},
arxivId = {1506.02351},
author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
eprint = {1506.02351},
file = {:home/eugene/Downloads/!articles/!important/1506.02351.pdf:pdf},
journal = {arXiv preprint arXiv:1506.02351},
number = {i},
pages = {1--12},
title = {{Stacked What-Where Auto-encoders}},
url = {http://arxiv.org/abs/1506.02351},
volume = {1},
year = {2015}
}
@article{He2016,
abstract = {Highlights • Novel pre-activation residual structure • Improved results using 1001-layer ResNet on CIFAR-10 and 200-layer on ImageNet • ResNet with Identity mapping • What if we break the identity shortcut? • Various types of shortcut connections},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:home/eugene/Downloads/!articles/!important/1603.05027.pdf:pdf},
isbn = {9783319464930},
issn = {0302-9743},
number = {1},
pages = {1--15},
pmid = {10463930},
title = {{Identity Mappings in Deep Residual Networks Importance of Identity Skip Connections Usage of Activation Function Analysis of Pre-activation Structure}},
url = {https://github.com/KaimingHe/resnet-1k-layers},
year = {2016}
}
@article{Lample2016,
abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.},
archivePrefix = {arXiv},
arxivId = {1609.05521},
author = {Lample, Guillaume and Chaplot, Devendra Singh},
eprint = {1609.05521},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lample, Chaplot - 2016 - Playing FPS Games with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv cs.AI},
number = {2015},
pages = {5521},
title = {{Playing FPS Games with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05521},
volume = {9},
year = {2016}
}
@article{Oord2016,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
eprint = {1601.06759},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oord, Kalchbrenner, Kavukcuoglu - 2016 - Pixel Recurrent Neural Networks.pdf:pdf},
isbn = {9781510829008},
journal = {International Conference on Machine Learning (ICML)},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {48},
year = {2016}
}
@article{alrecovering,
author = {Al-Halah, Ziad and Tapaswi, Makarand and Stiefelhagen, Rainer},
title = {{Recovering the Missing Link: Predicting Class-Attribute Associations for Unsupervised Zero-Shot Learning}}
}
@inproceedings{torch,
author = {Collobert, R and Kavukcuoglu, K and Farabet, C},
booktitle = {BigLearn, NIPS Workshop},
title = {{Torch7: A Matlab-like Environment for Machine Learning}},
year = {2011}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
pages = {1--9},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {27},
year = {2014}
}
@article{Ng2011,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2011 - Sparse autoencoder.pdf:pdf},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lecture notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{DBLP:journals/corr/ZhangW15a,
author = {Zhang, Dongxu and Wang, Dong},
journal = {CoRR},
title = {{Relation Classification via Recurrent Neural Network}},
url = {http://arxiv.org/abs/1508.01006},
volume = {abs/1508.0},
year = {2015}
}
@article{Szegedy2015,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Hinton2006,
author = {Hinton, G and Salakhutdinov, Ruslan},
file = {:home/eugene/Downloads/!articles/!important/science.pdf:pdf},
journal = {Science},
number = {July},
pages = {504--507},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@article{DBLP:journals/corr/SantosXZ15,
author = {dos Santos, C$\backslash$'$\backslash$icero Nogueira and Xiang, Bing and Zhou, Bowen},
journal = {CoRR},
title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1504.06580},
volume = {abs/1504.0},
year = {2015}
}
@article{Anandkumar16,
abstract = {Local search heuristics for non-convex optimizations are popular in applied machine learning. However, in general it is hard to guarantee that such algorithms even converge to a local minimum, due to the existence of complicated saddle point structures in high dimensions. Many functions have degenerate saddle points such that the first and second order derivatives cannot distinguish them with local optima. In this paper we use higher order derivatives to escape these saddle points: we design the first efficient algorithm guaranteed to converge to a third order local optimum (while existing techniques are at most second order). We also show that it is NP-hard to extend this further to finding fourth order local optima.},
archivePrefix = {arXiv},
arxivId = {1602.05908},
author = {Anandkumar, Anima and Ge, Rong},
eprint = {1602.05908},
file = {:home/eugene/Downloads/!articles/!important/1602.05908.pdf:pdf},
journal = {COLT},
keywords = {Non-convex,Optimization,Saddle Point},
pages = {1--21},
title = {{Efficient Approaches For Escaping Higher Order Saddle Points In Non-Convex Optimization}},
url = {http://arxiv.org/abs/1602.05908},
volume = {49},
year = {2016}
}
@article{Bengio2015,
abstract = {We introduce a predictive objective function for the rate aspect of spike-timing dependent plasticity (STDP), i.e., ignoring the effects of synchrony of spikes but looking at spiking {\{}$\backslash$em rate changes{\}}. The proposed weight update is proportional to the presynaptic spiking (or firing) rate times the {\{}$\backslash$em temporal change{\}} of the integrated postsynaptic activity. We present an intuitive explanation for the relationship between spike-timing and weight change that arises when the weight change follows this rule. Spike-based simulations agree with the proposed relationship between spike timing and the temporal change of postsynaptic activity. They show a strong correlation between the biologically observed STDP behavior and the behavior obtained from simulations where the weight change follows the gradient of the predictive objective function.},
archivePrefix = {arXiv},
arxivId = {1509.05936},
author = {Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhai},
eprint = {1509.05936},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2015 - STDP as presynaptic activity times rate of change of postsynaptic activity.pdf:pdf},
journal = {arXiv},
number = {2000},
pages = {1--15},
title = {{STDP as presynaptic activity times rate of change of postsynaptic activity}},
url = {http://arxiv.org/abs/1509.05936},
year = {2015}
}
@inproceedings{pennington2014glove,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://www.aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Bunescu2005139,
annote = {Information Extraction and Summarization from Medical Documents},
author = {Bunescu, Razvan and Ge, Ruifang and Kate, Rohit J and Marcotte, Edward M and Mooney, Raymond J and Ramani, Arun K and Wong, Yuk Wah},
doi = {10.1016/j.artmed.2004.07.016},
issn = {0933-3657},
journal = {Artificial Intelligence in Medicine},
keywords = {Information extraction,Machine learning,Medline,Protein interactions,Text mining},
number = {2},
pages = {139--155},
title = {{Comparative experiments on learning information extractors for proteins and their interactions}},
url = {http://www.sciencedirect.com/science/article/pii/S0933365704001319},
volume = {33},
year = {2005}
}
@inproceedings{kim2011overview,
author = {Kim, Jin-Dong and Pyysalo, Sampo and Ohta, Tomoko and Bossy, Robert and Nguyen, Ngan and Tsujii, Jun'ichi},
booktitle = {Proceedings of the BioNLP Shared Task 2011 Workshop},
organization = {Association for Computational Linguistics},
pages = {1--6},
title = {{Overview of BioNLP shared task 2011}},
year = {2011}
}
@article{Greff2016,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2016 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
year = {2016}
}
@article{Wang2017,
archivePrefix = {arXiv},
arxivId = {1701.05294},
author = {Wang, Chen and Yuan, Junsong and Xie, Lihua},
eprint = {1701.05294},
file = {:home/eugene/Downloads/!articles/!important/1701.05294.pdf:pdf},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
title = {{Non-Iterative SLAM: A Fast Dense Method for Inertial-Visual SLAM}},
year = {2017}
}
@article{Palogiannidi2016,
abstract = {We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the sub-task B. Our system comprises of multiple in-dependent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way. The novelty of the system is the employment of a topic modeling approach in order to adapt the semantic-affective space for each tweet. In addition, significant enhancements were made in the main system dealing with the data pre-processing and feature extraction including the employment of word embeddings. Each model is used to predict a tweet's sentiment (positive, negative or neutral) and a late fusion scheme is adopted for the final decision.},
author = {Palogiannidi, Elisavet and Kolovou, Athanasia and Christopoulou, Fenia and Kokkinos, Filippos and Iosif, Elias and Malandrakis, Nikolaos and Papageorgiou, Harris and Narayanan, Shrikanth and Potamianos, Alexandros},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palogiannidi et al. - 2016 - Tweester at SemEval-2016 Task 4 Sentiment Analysis in Twitter using Semantic-Affective Model Adaptation.pdf:pdf},
pages = {160--168},
title = {{Tweester at SemEval-2016 Task 4 : Sentiment Analysis in Twitter using Semantic-Affective Model Adaptation}},
year = {2016}
}
@article{Silver,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Den, George Van and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/eugene/Downloads/!articles/!important/deepmind-mastering-go.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
number = {1},
pages = {1--37},
pmid = {26819042},
title = {{Mastering the Game of Go with Deep Neural Networks and Tree Search}}
}
@article{bordes2012,
author = {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
file = {:home/eugene/Downloads/!articles/!important/bordes12.pdf:pdf},
pages = {127--135},
title = {{Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing}},
volume = {22},
year = {2012}
}
@article{GoogleResearch2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {GoogleResearch},
eprint = {arXiv:1603.04467v2},
file = {:home/eugene/Downloads/!articles/!important/45166.pdf:pdf},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Szegedy2016a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
file = {:home/eugene/Downloads/!articles/!important/1512.00567.pdf:pdf},
isbn = {9781617796029},
issn = {08866236},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2016}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1406.2661v1},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Mahendran2014,
abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
archivePrefix = {arXiv},
arxivId = {1412.0035},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
doi = {10.1109/CVPR.2015.7299155},
eprint = {1412.0035},
file = {:home/eugene/Downloads/!articles/!important/1412.0035.pdf:pdf},
isbn = {9781467369640},
issn = {9781467369640},
title = {{Understanding Deep Image Representations by Inverting Them}},
url = {http://arxiv.org/abs/1412.0035},
year = {2014}
}
@article{Krizhevsky2011,
abstract = {We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple dierent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.},
author = {Krizhevsky, Alex and Hinton, Ge},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Hinton - 2011 - Using Very Deep Autoencoders for Content-Based Image Retrieval.pdf:pdf},
isbn = {9782874190445},
journal = {Proceedings of the European Symposium on Artificial Neural Networks (ESANN)},
pages = {1--7},
title = {{Using Very Deep Autoencoders for Content-Based Image Retrieval}},
year = {2011}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? During image downsampling information is lost, making super-resolution a highly ill-posed inverse problem with a large set of possible solutions. The behavior of optimization-based super-resolution methods is therefore principally driven by the choice of objective function. Recent work has largely focussed on minimizing the mean squared reconstruction error (MSE). The resulting estimates have high peak signal-to-noise-ratio (PSNR), but they are often overly smoothed, lack high-frequency detail, making them perceptually unsatisfying. In this paper, we present super-resolution generative adversarial network (SRGAN). To our knowledge, it is the first framework capable of recovering photo-realistic natural images from 4 times downsampling. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss function motivated by perceptual similarity instead of similarity in pixel space. Trained on 350K images using the perceptual loss function, our deep residual network was able to recover photo-realistic textures from heavily downsampled images on public benchmarks.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
eprint = {1609.04802},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:pdf},
journal = {arXiv},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2016}
}
@article{Makhzani2013,
abstract = {Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.},
archivePrefix = {arXiv},
arxivId = {1312.5663},
author = {Makhzani, Alireza and Frey, Brendan},
eprint = {1312.5663},
file = {:home/eugene/Downloads/!articles/!important/1312.5663.pdf:pdf},
keywords = {()},
title = {{k-Sparse Autoencoders}},
url = {http://arxiv.org/abs/1312.5663},
year = {2013}
}
@article{Dosovitskiy2015,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Brox, Thomas},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosovitskiy, Springenberg, Brox - 2015 - Learning to generate chairs with convolutional neural networks.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1538--1546},
pmid = {18267787},
title = {{Learning to generate chairs with convolutional neural networks}},
volume = {07-12-June},
year = {2015}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
file = {:home/eugene/Downloads/!articles/!important/1602.07261.pdf:pdf},
journal = {Arxiv},
pages = {12},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
year = {2016}
}
@article{Masci2011,
abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
author = {Masci, Jonathan and Meier, Ueli and Cireşan, Dan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1007/978-3-642-21735-7_7},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masci et al. - 2011 - Stacked convolutional auto-encoders for hierarchical feature extraction.pdf:pdf},
isbn = {9783642217340},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {auto-encoder,classification,convolutional neural network,unsupervised learning},
number = {PART 1},
pages = {52--59},
title = {{Stacked convolutional auto-encoders for hierarchical feature extraction}},
volume = {6791 LNCS},
year = {2011}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:home/eugene/Downloads/!articles/!important/1606.01540.pdf:pdf},
journal = {arXiv},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Vinyals2015,
abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent ap- proaches such as sequence-to-sequence [1] and Neural Turing Machines [2], be- cause the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems – finding planar convex hulls, computing Delaunay triangu- lations, and the planar Travelling Salesman Problem – using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems. 1},
archivePrefix = {arXiv},
arxivId = {1506.03134},
author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1506.03134},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals, Fortunato, Jaitly - 2015 - Pointer Networks.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems 2015},
pages = {1--9},
title = {{Pointer Networks}},
year = {2015}
}
@article{Cadena2015,
archivePrefix = {arXiv},
arxivId = {1606.05830v2},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Reid, Ian D and Leonard, John J},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830v2},
file = {:home/eugene/Downloads/!articles/!important/1606.05830.pdf:pdf},
isbn = {9781479936847},
issn = {1552-3098},
number = {July},
pages = {1--27},
pmid = {6576973927449638915},
title = {{Past , Present , and Future of Simultaneous Localization And Mapping : Towards the Robust-Perception Age}},
year = {2015}
}
@inproceedings{cheng2016wide,
author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Others},
booktitle = {Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
organization = {ACM},
pages = {7--10},
title = {{Wide {\&} Deep Learning for Recommender Systems}},
year = {2016}
}
@article{Huang2016a,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74{\%} test error on CIFAR-10, 19.25{\%} on CIFAR-100 and 1.59{\%} on SVHN).},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
eprint = {1608.06993},
file = {:home/eugene/Downloads/!articles/!important/1608.06993.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
pmid = {211888},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@article{Hernandez2016,
abstract = {This paper describes Team Delft's robot, which won the Amazon Picking Challenge 2016, including both the Picking and the Stowing competitions. The goal of the challenge is to automate pick and place operations in unstructured environments, specifically the shelves in an Amazon warehouse. Team Delft's robot is based on an industrial robot arm, 3D cameras and a customized gripper. The robot's software uses ROS to integrate off-the-shelf components and modules developed specifically for the competition, implementing Deep Learning and other AI techniques for object recognition and pose estimation, grasp planning and motion planning. This paper describes the main components in the system, and discusses its performance and results at the Amazon Picking Challenge 2016 finals.},
archivePrefix = {arXiv},
arxivId = {1610.05514},
author = {Hernandez, Carlos and Bharatheesha, Mukunda and Ko, Wilson and Gaiser, Hans and Tan, Jethro and van Deurzen, Kanter and de Vries, Maarten and {Van Mil}, Bas and van Egmond, Jeff and Burger, Ruben and Morariu, Mihai and Ju, Jihong and Gerrmann, Xander and Ensing, Ronald and {Van Frankenhuyzen}, Jan and Wisse, Martijn},
eprint = {1610.05514},
file = {:home/eugene/Downloads/!articles/!important/1610.05514.pdf:pdf},
keywords = {deep learning,grasping,motion planning,robotic system,warehouse automation},
pages = {1--13},
title = {{Team Delft's Robot Winner of the Amazon Picking Challenge 2016}},
url = {http://arxiv.org/abs/1610.05514},
year = {2016}
}
@article{Vincent2010,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpass-ing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordi-nary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
file = {:home/eugene/Downloads/!articles/!important/vincent10a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {autoencoders,deep belief networks,deep learning,denoising,unsupervised feature learning},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Fenton2009,
author = {Fenton, Andr{\'{e}} A and Kao, Hsin-yi and Neymotin, Samuel A and Olypher, Andrey and Lytton, William W and Ludvig, Nandor},
doi = {10.1523/JNEUROSCI.2862-08.2008.Unmasking},
file = {:home/eugene/Downloads/!articles/!important/nihms77418.pdf:pdf},
keywords = {ca1,extracellular recording,hippocampal function,hippocampus,neuronal ensembles,place cells,spatial cognition},
number = {44},
pages = {11250--11262},
title = {{NIH Public Access}},
volume = {28},
year = {2009}
}
@article{Guo2016,
abstract = {Abstract Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks. },
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S},
doi = {10.1016/j.neucom.2015.09.116},
file = {:home/eugene/Downloads/!articles/!important/Deep-learning-for-visual-understanding{\_}-A-review.pdf:pdf},
issn = {0925-2312},
journal = {Neurocomputing},
pages = {27--48},
title = {{Neurocomputing Deep learning for visual understanding : A review}},
volume = {187},
year = {2016}
}
@article{Whitney2016,
abstract = {We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.},
archivePrefix = {arXiv},
arxivId = {1602.06822},
author = {Whitney, William F and Chang, Michael and Kulkarni, Tejas and Tenenbaum, Joshua B},
eprint = {1602.06822},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Whitney et al. - 2016 - Understanding Visual Concepts with Continuation Learning.pdf:pdf},
journal = {arXiv},
pages = {1--4},
title = {{Understanding Visual Concepts with Continuation Learning}},
url = {http://arxiv.org/abs/1602.06822},
year = {2016}
}
@inproceedings{zeng2015distant,
author = {Zeng, Daojian and Liu, Kang and Chen, Yubo and Zhao, Jun},
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal},
pages = {17--21},
title = {{Distant supervision for relation extraction via piecewise convolutional neural networks}},
year = {2015}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:home/eugene/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2015 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--15},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
