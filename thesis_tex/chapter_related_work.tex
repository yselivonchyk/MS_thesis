% !TEX root = ./thesis.tex

\chapter{Related work}
\label{ch:rewo}
Stacked conv AE \cite{Masci2011} introduce Convolutional Auto-Encoder for hierarchical feature extraction.
They claim, that initializing CNN with convolutional layers of trained CAE consistently increases perfromance of a CNN.

VAE. Tutorial on VAE \cite{Doersch2016}

Rationalizing Neural Prediction \cite{Lei2016}


\textit{recommended by tutors}
NG Sparse AE  \cite{Ng2011} introduces notion of sparcity properly
Learning Deep Architectures for AI \cite{Bengio2009}
Stacked convolutional auto-encoders for hierarchical feature extraction \cite{Masci2011}

Spatial Transformer Networks \cite{Jaderberg2015}

Loop closure detection for visual SLAM systems using deep neural networks \cite{Gao2015}
Authors build a denoising autoencoder with sparce objective adding continuity objective.
Continuity objective enforces L2 similaraty between extracted features for consequtive frames. They use dataset: freiburg2 slam.
Deep Convolutional Inverse Graphics Network \cite{Kulkarni2015}.
Based on idea of VAE XXX build a network that extracts features related to spatial transformations of 3D objects.

Understanding Visual Concepts with Continuation Learning \cite{Whitney2016}
Authors propose a neural network archtecture for extracting a subset of highly interpretable features
from image sequencies.

Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion \cite{Vincent2010}

Loop closure:
Loop Closure Detection for Visual SLAM Using PCANet Features.
Unsupervised learning to detect loops using deep neural networks for visual SLAM system.
VLAD-Based Loop Closure Detection For Monocular SLAM.
\cite{Xia2016, Gao2015a, Huang2016}



Autoencoders

Stacked What-Where Auto-encoders \cite{Zhao2015}
Auto-Encoding Variational Bayes \cite{Kingma2013}

Deep Learning \cite{LeCun2015}

Notion of sparcity


Localization issues
