% !TEX root = ./thesis.tex

\chapter{Introduction}
\label{ch:intro}

% Outline:
% - successes of Neural network models
% - specifically for spacial related field (spatial transformer network, video thing)
% - VizDoom and
% - we propose
% - potential usages of the proposed system: SLAM, loop closure detection
% - analyses description
%
% Space-time video completion \cite{Wexler2004}
% % Deep learning for visual understanding: A review \cite{Guo2016}
%
% Loop closure detection for visual SLAM systems using deep neural networks \cite{Gao2015}
% Authors build a denoising autoencoder with sparse objective adding continuity objective.
% Continuity objective enforces L2 similarity between extracted features for consecutive frames. They use dataset: freiburg2 slam.
%
% Loop Closure Detection for Visual SLAM Using PCANet Features.
% Unsupervised learning to detect loops using deep neural networks for visual SLAM system.
% VLAD-Based Loop Closure Detection For Monocular SLAM \cite{Xia2016, Gao2015a, Huang2016}


Localization tasks represent a significant challenge in the area of artificial intelligence (AI).
In particular, localization is extremely important to such AI areas as robotics, self-driving vehicles, and micro-surgery, to name a few \cite{Wang2017, Mountney2006}.
Tasks such as simultaneous localization and mapping (SLAM) \cite{Cadena2015, Zikos2016}, loop closure detection \cite{Xia2016, Gao2015a, Huang2016}, and correspondence learning \cite{Boscaini2016} all try to solve some form of localization problem.

We can generally define localization as the task of extracting, tracking or predicting an object's position in some environment from available sensory data. As, for example, for self-driving vehicles, it might be important to track and predict the relative position of pedestrians within the field of view from visual data \cite{Dollar2009}. The same kind of data can be used to extract positions and dimensions of objects manipulated by automated robots \cite{Hernandez2016}. Visual data comes in the form of sequences of discretized images containing 3 color channels (RGB) and often an additional channel with depth information (RGBD) \cite{Long2015}. Such models are typically trained on annotated datasets.

In this work we focus on unsupervised localization in artificial environments such as computer games. Recent successes in reinforcement learning \cite{Silver, Lample2016} has proven that, given access to computational resources and sufficient amounts of training data, even ill-defined machine-learning problems can be solved effectively. In this work we would like to take advantage of data-generation techniques created for reinforcement learning \cite{Brockman2016, Kempka2016}, in order to solve the problem of localization in an unsupervised manner. More specifically, we are going to use the 3D engine of a first-person shooter (FPS) computer game \texttt{Doom II} in order to generate a sufficient amount of visual data for training.

We propose a model that extracts positional information about the player from the current view.
Visual data is typically high-dimensional, and each image may contain tens of thousands of features. Meanwhile, we know that the current view can be compactly and unambiguously described by the current position of the player and the direction of the view. Therefore, we can use the position and the direction of the view as a low-dimensional, latent representation of high-dimensional visual data. Furthermore, the continuous nature of these latent features allows us to state that there exists a low-dimensional manifold on which all playersâ€™ movements remain continuous. Therefore, we aim to build a learning model for extracting such a manifold from visual data.

\begin{figure}[t!]
	\centering
	\subfigure[Example video data, recorded from the first person view of a player in the shooter game  \texttt{Doom II}.]{
  \includegraphics[width=.99\textwidth]{sprite2_2.png}
	}\\
	\subfigure[Example 3-dimensional embedding produced by the model.]{
    	\includegraphics[width=.45\textwidth]{cmp2/cnn_3.png}
	}
	\hfill
	\subfigure[Visualization of the actual player's path.]{
    \includegraphics[width=.45\textwidth]{path.png}
	}
    	\caption{Visualization of spatial information extraction from a video signal by an unsupervised autoencoder-based model. The model takes a sequence of RGBD images as input (top), observed by the actor while walking along some path. The model projects each frame into a lower-dimensional space in order to extract information relevant to the position of the player (bottom left). Sequential frames are displayed in similar colors. The bottom-left picture schematically draws the actual trajectory of the player.}
    	\label{fig:intro_ex}
\end{figure}

We propose a deep learning technique that learns a mapping of unlabeled, high-dimensional visual data in RGBD format into a continuous, low-dimensional latent feature space (see figure \ref{fig:intro_ex}(a)). We describe a training method that allows for robust learning of such a mapping, even for extremely high compression ratios of 10000:1, although with unavoidable information loss. Furthermore, we describe a set of regularization techniques, which can reveal latent spatial relations in the input data and, at least for inputs of moderate complexity, build approximate predictions in the latent feature space.

Several models have been successfully applied to unlabeled data, allowing for the construction of a dense manifold representation of some visual concepts \cite{Li2015, Kingma2013, Goodfellow2014}.
While these techniques succeeded in encoding data in a lower-dimensional space of independent features, they make no assumption about the nature of these features.
We expect the extremely low-dimensional representation of the spatially-related data to be tightly coupled with that spatial information.
To enforce this expectation, we apply additional constraints to the model to preserve geometrical qualities in the latent features.
Existing research on extracting interpretable features suggests that such extraction is possible, though it might significantly complicate the training process \cite{Lei2016, Kulkarni2015}.

An ultimate goal of this project can be viewed as a direct and inverse graphics engine in the form of an autoencoder. In such a model, the encoder network performs the mapping of current visual information into the coordinate space; the decoder learns to reconstruct the image from the positional information.
The goal can be described as achieving an autoencoder objective in the form of perfect image reconstruction, while producing a dense continuous spatial manifold in the latent feature space.
Having successfully achieved this goal, we will be able to produce the player's view, given the position, and vice-versa: to determine the possible position of the player by the current view.
This model can behave as an ultimate solution for SLAM, correspondence and loop detection problems.
We understand that this goal is unlikely to be fully achieved, given limited computational resources, the discrete nature of actual training data and the often non-static characteristics of real-world environments.

Later chapters are organized as follows.
We continue the current discussion in chapter \ref{ch:rewo} by describing recent advances in research, relevant to the task at hand.
In chapter \ref{ch:tede} we provide technical details about artificial neural networks, their training techniques, and common complications of the training process.
Chapter \ref{ch:mode} describes our learning method and additional model constraints, along with the underlying motivation.
Finally, in chapter \ref{ch:eval} we explore the advantages of our method when applied to actual data.
Chapter \ref{ch:conc} deals with the results of our research and outlines possible future work.
