% !TEX root = ./thesis.tex

\chapter{Discussion}
\label{ch:conc}

In this work we have proposed a deep-learning technique for mapping sequences of visual data into a manifold space while preserving key spatial relations.
We provide a pre-training technique to improve robustness of the training for high compression rates.
The effectiveness of our approach is analyzed through tasks of varying complexity.
Experiments showed that our method allows us to effectively learn some trivial trajectories and recognize structure in more complex cases.
Learned solutions reveal local spatial relation in the data and allow for the identification of self-intersecting trajectories.
We showed that, while training from exclusively visual data without any additional information, our technique extracts low-dimensional features, allowing for approximate prediction of the future position of the subject (player).

Our model in its current state appears to be highly sensitive to the complexity of the underlying concept. While using pixel-wise error as the main mean of gradient-based training, some common transformations, such as the ones resulting from changing the direction of the view, cause a sharp decrease in the quality of the extracted features. In the scope of future work, we propose splitting the feature space into two subspaces, one for handing more precise and lower-dimensional positional information and the other for maintaining information about changing the direction of the view. Such separation of concerns would allow us to achieve higher precision in positional embedding, while allowing for a higher dimensionality for representing direction of the view. This type of model would require some additional input apart from purely visual data, yet would also be accessible through our current data collection method.
