% !TEX root = ./thesis.tex

\chapter{Technical background}
\label{ch:tede}

This chapter provides technical details relevant to the method proposed in chapter \ref{ch:mode}.
In subsection \ref{ch:nn} we provide an overview of artificial neural networks and related optimization techniques.
Subsection \ref{ch:ae} describes several unsupervised learning techniques for artificial neural networks also known as autoencoders.

\section{Artificial Neural Networks}
\label{ch:nn}
An artificial neural network is a mathematical model inspired by biological brain structure.
A neural network comprises of a set of interacting computational units, called neurons.
In practice, neurons' computation contains a linear transformation of inputs followed by an activation function.
Sigmoid, ReLU and identity functions are among most commonly used activation functions.

Neural networks are organized in layers.
A layer can be viewed as a set of neurons that rely on same inputs and have similar behavior i.e. activation function, output size, regularization parameters.
We also distinguish auxiliary layers that perform some transformation of the input data.
Scaling layer, applying nonlinearity function and pooling layer are one of many examples of such layers.

Major neural networks groups are distinguished between each other by structure of connections between neurons.
Feedforward neural networks prohibit cyclic connection and loops in the network structure, while Recurrent neural networks have no such restriction.

In subsection \ref{ch:ffnn} we are going to describe basic building blocks of feedforward neural networks.
Subsection \ref{ch:cnn} describes a special case of feedforward neural network with parameter sharing.
Finally, in subsection \ref{ch:opt} we discuss basic concepts of training artificial neural networks.

% \subsection{Artificial Neural Network}
\subsection{Feedforward Neural Networks}
\label{ch:ffnn}

Let's take a look at the simplest case of an artificial neural network: the perceptron.
The single layer perceptron performs a linear transformation of the input values and subsequently applies some activation function $f$:
\begin{equation}\label{eq:per}
  y = f(\sum_{i=1}^N W_ix_i + b) = f(Wx+b)
\end{equation}
where $W$ is a column vector of perceptron parameters and $f$ is an activation function. The structure of the perceptron is depicted in figure \ref{fig:perc}.

\input{auxilary/perceptron}

By combining several perceptrons in one set we can produce multiple output values of $y$.
For simplicity, we will stack parameters of $j$ distinct perceptrons that use the same input of size $i$ into matrices $W$ and $b$, where $W$ is rectangular matrix of size $[i,j]$ and $b$ is a column vector of size $j$.
In this case equation $\ref{eq:per}$ remains the same for column vector $y$ of size $j$. A set of perceptron-like neurons that perform computation on the same input forms a single \textit{fully connected} layer of a multilayer neural network.

When we combine several layers together we would refer to input vector $x$ as \textit{input layer} and to the last layer of the network producing final output $y$ as \textit{output layer}. All intermediate layers between input and output are called \textit{hidden layers}. We would refer to the depth of the network as to number of computational units on the longest path connecting input and output layers.

It has been shown, that artificial neural network, under mild assumption about the form of the activation function, can act as universal approximator even using a single layer of hidden units of finite size \cite{Debao1993}. Yet, some approximation would require at least exponentially many hidden units in number of inputs $||h||=2^{||x||}$ \cite{Pascanu2014} while using only a single hidden layer. Furthermore, architecutre of $k+1$ layers can be exponentially less complex that architecture of $k$ layers \cite{Bengio2009a}.
Hence, many researchers are developing neural networks with hundreds of layers to allow more compact representations of complex functions \cite{He2015, Srivastava2015}.

Expressivity of the functions that can be approximated by a neural network comes at a cost.
Extensively complex models are prone to overfitting. In other words, models can show excellent performance on the training data while providing poor solutions on yet unseen inputs. In the worst case, an overparameterized model is capable of learning exact mapping between inputs and desired output values without learning useful concepts.

To reduce overfitting several techniques can be used. Reducing model capacity, collecting or generating more training data and regularizing the model are among the most widely used in the context of neural networks.

Using convolutional neural networks is one of the options to reduce capacity of the model by using shared model weights.

Xaviere initializatin \cite{Glorot2010}

\input{auxilary/activation_functions}

% \subsection{Artificial Neural Network}
\subsection{Convolutional Neural Networks}
\label{ch:cnn}

The convolutional neural network is a special kind of feedforward neural network that takes advantage of structural organization of the data. This technique allows to decrease the number of parameters of the model when applied to structured data.

Many types of data contain redundant information .i.e. information that, when ignored, can be recovered from the remaining data.
For instance, we can rather easily reconstruct the skipped word in the sentence "\textit{Clouds are in the \_\_}" as well as we can recognize an object on an image when some pixels are altered or missing.
In other words, components of textual or visual data are often interdependent and contain duplicated information.

Convolutional neural networks use small specially adjacent parts of data as neuron inputs. Multiple neurons reuse the same set of parameters $K$, called \textit{kernel}, on distinct patches of the input. All convolutional neurons depending on the same kernel $K$ form a \textit{convolutional filter}. A set of outputs produced by a single convolutional filter is called a \textit{feature map}.



For a 3-dimensional image $I$ of size $H \times W \times C$, where $H$,$W$,$C$ are height, width and number of color channels correspondingly, applying 2-dimensional convolutional filter $K$ of size $w \times h$ would result in a feature map of size $(H-h+1) \times (W-w+1) \times 1$. Kernel $K$ would use $w \times h \times C$ parameters. Convolution operation can be described as:
\begin{equation}\label{eq:conv}
  (I \cdot K)_{x, y} = \sum_a \sum_b \sum_c K_{a,b,c} I_{x+a, y+b,c}
\end{equation}
where ${x, y}$ and ${a,b}$ are two-dimensional indices of feature map and convolution correspondingly and follow next constrains:
\begin{equation*}
  \begin{aligned}
  &1 \leq  x \leq H-h+1, \\
  &1 \leq  y \leq W-w+1, \\
  &1 \leq  a \leq h, \\
  &1 \leq  b \leq w, \\
  &1 \leq  c \leq C.
\end{aligned}
\end{equation*}

\begin{figure}
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{cnn.png}
  \caption{Illustration of applying convolutional filter of size 3x3 to a single square patch. Output feature map is padded to preserve same shape as input data. Picture taken from [community.arm.com]}
  \label{fig:cnn}
\end{figure}

To preserve width and hight of the feature map $(I \cdot K)$ equivalent to the source image we can add padding along the edges of the input. We can duplicate the same values or use zeros as a padding.
Further, we can apply $k$ distinct convolutional filters to produce output volume of size $x \times y \times k$.

Output of convolution \ref{eq:conv} is often summed with an additional bias $b$ before further processing.
The bias is shared across all convolutional neurons depending on the same kernel.
Value $(I \cdot K)_{x, y}+b$, called \texttt{preactivation}, is forwarded to activation function $\sigma$.

The activation operation can be followed by a pooling operation.
Pooling replaces the output of the convolution layer with summary statistics of nearby outputs.
Max-pooling is one example of such an operation.

\subsubsection{Max-pooling layer}

Max-pooling is a pooling operation applied independently to every feature map.
Max-pooling replaces the output of $\sigma((I \cdot K)_{x, y}+b)$ with a maximum value over a small region of size $w$, $h$ centered at point $(x,y)$.

We can use convolutional and pooling operations in \texttt{strided} mode. When using strides instead of applying operation to every unique position $(x, y)$ we can skip over some indices. Applying strides greater than $1 \times 1$ results in decrease in size of the feature map. For an image of size $H \times W \times C$ and max-pooling operation with stride $h \times w$ we can calculate size of the output feature map as follows $H_o \times W_o \times C $:
\begin{equation*}
  \begin{aligned}
  & W_o = \ceil[\bigg]{\frac{W}{w}} \\
  & H_o = \ceil[\bigg]{\frac{H}{h}} \\
\end{aligned}
\end{equation*}

Downsampling is essential to reduce computational complexity of the network.
Downsampling with pooling layers improves positional tolerance of convolutional networks \cite{Scherer2010}.
Strided pooling is preferred to strided convolution operation for downsampling. Strided pooling layers are shown to work better in practice, allegedly, due to greater amount of valuable information preserved during downsampling.

Max-pooling operation does not rely on any internal parameters. During training, the position of the maximum element has to be stored. During backpropagation gradients are applied only to the position of the maximum value. Non-maximum elements of the input feature map do not contribute to the total error since they are not used by the upper layers of the network.

\subsection{Network parameters learning}
\label{ch:opt}

TODO: rework with empirical risk minimization in mind. introduce notion of generalization error

Even though a multilayer neural network can approximate large family of compact functions arbitrary well \cite{Debao1993}, finding the right parameters for such approximation is non-trivial.
Lets consider some error function $L(y, f(x, \theta))$ that calculates error between expected values $y$ and the output of the function $f$, realized by a neural network depending on parameters $\theta$. Finding the solution for a general neural network is equivalent to solving next optimization problem:

\begin{equation}\label{eq:optnn}
  \hat{\theta} = \argmin_{\theta \in \Bbb{R}^D} J(\theta) = \argmin_{\theta \in \Bbb{R}^D} \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i, \theta))
\end{equation}
where $D$ is equivalent to number of parameters $||\theta||$ and $N$ is the size of the training set.

Problem \ref{eq:optnn} has no closed-form solution and finding optimal set of parameters $\hat{\theta}$ is an NP-hard problem \cite{Anandkumar16}.
In practice, gradient descent methods are used to find good parametrization $\hat{\theta}$ of the neural network.

Gradient descent methods suggests to iteratively update parameters $\theta_t$ each time taking a step $\eta$ towards the direction of a better solution $\theta_{t+1}=\theta_t - \eta \nabla_\theta \theta$ \cite{Cauchy1847}.
We use vector of partial derivatives $\nabla_\theta J(\theta)=\{ \frac{\partial J(\theta)}{\partial w_1}, \ldots, \frac{\partial J(\theta)}{\partial w_N} \}$ of function $J(\theta)$ at point $\theta_t$ as the the direction towards a better solution. Gradient descent method requires both functions $L(y, \hat{y})$ and $f(x, \theta)$ to be differentiable. Note, that differentiability of function $f(x, \theta)$ in $\theta$ implies differentiability of the activation function \ref{eq:per}. Algorithm pseudocode is shown on listing \ref{alg:bp}.

\input{auxilary/backprop}

Gradient based methods do not guarantee to find the optimal solution.
Traing can stop in a local minima ($\theta \leq \theta + \eta \delta | \forall \delta \text{ s.t. } |\delta|^2 \leq \epsilon$) or a saddle point.
These cases are shown on figure \ref{fig:critical}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{minmaxsaddle.png}
  \caption{Critical points of a loss function $f(x)$, where $x \in \Bbb{R}^2$.}
  \label{fig:critical}
\end{figure}

In practice, solutions produced by gradient based methods have proven to have satisfying qualities \cite{Szegedy2016,He2015}.
This might be due to the properties of high-dimensional non-convex error surfaces. The study by Y.Dauphin et al. \cite{Dauphin14} concludes, that saddle points and local minima on such surfaces are very likely to be a good approximation of the global minima.

\subsubsection{Stochastic gradient descent with mini-batches}

Algorithm \ref{alg:bp} requires full path through the complete training set on each iteration.
This might be computationally wasteful for large datasets. Instead, we can perform optimization iteratively only considering subsets of the training set, drawn i.d.d. from the training set:

\begin{equation*}\label{eq:sopt}
  \begin{aligned}
  & \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i, \theta)) = \E_{i \sim U(1,N) } L(y_i, f(x_i, \theta)) \\
  & \E_{i \sim U(1,N) } L(y_i, f(x_i, \theta))= \frac{1}{M} \E_{m \sim U^M(1,N) } \sum_{i \in m} L(y_i, f(x_i, \theta))
  \end{aligned}
\end{equation*}

As a result, we can use the stochastic procedure of selecting $M$ inputs from the training dataset to use in each training iteration.
Subset of inputs selected this way are called \textbf{mini-batches}. Updated algorithm is shown on listing \ref{alg:sbp}.

\input{auxilary/minibatch}

\subsubsection{Backpropagation algorithm}

\subsubsection{Adam update rule}

Training time of gradient descent based algorithms can be reduced by applying adaptive learning rates \cite{Kingma2015}.
\textit{Adam update rule} or simply \textit{Adam} is one of the algorithms that allows to reduce training time of the gradient descent.

The Adam update rule maintains exponential moving average of the gradients $m_t$ and the squared gradient $v_t$.
Hyperparameters $\beta_1, \beta_2$ set the exponential decay rates for both sets of variables.
The update rule for exponential moving average is as follows:
\begin{equation*}
  \begin{aligned}
    & m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t \\
    & v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2
  \end{aligned}
\end{equation*}

Variables $m_t$ and $v_t$ are estimating 1st and 2nd moments of the gradients.
We can therefore rewrite update rule:
\begin{equation}
  \theta_t \gets \theta_{t-1} - \frac{\eta m_t}{\sqrt{v_t} + \epsilon}
\end{equation}
where $\epsilon$ is a small guard value that prevents the divisor from getting too close to zero.

Exponential moving averages $m_t$ and $v_t$ are initialized with zeros.
This leads to zero-bias of the estimators during initial phase of the training.
To accelerate the learning process during first iterations of algorithm \ref{alg:sbp} a time dependent regularization is applied:
\begin{equation*}
  \begin{aligned}
    & \hat{m} = m / (1-\beta_1^t) \\
    & \hat{v} = v/(1-\beta_2^t)
  \end{aligned}
\end{equation*}
where $t$ is a variable tracking current iteration number.

The algorithm requires memory overhead proportional to the number of parameters $|\theta|$. Listing \ref{alg:adam} shows the full algorithm of Adam update rule written in pseudocode.

\input{auxilary/adam}

\subsection{Regularization}

\subsubsection{Bagging}
\subsubsection{Dropout}

Applying an ensemble of models guarantees to perform not worse than any of ensemble members but the computational complexity of the training and evaluation grows linearly with the size of the ensemble. This might play a limiting factor on the size of the ensemble, especially when training of even a single model is computationally expensive.

The \textit{dropout} technique allows to approximately achieve performance of an ensemble of heterogenious models while only marginaly increasing training complexity \cite{Srivastava2014}.
The dropout authors propose to ignore a randomly chosen subset of non-output neurons during training. A new subset of units is selected for each mini-batch or even for each training example.

Training a model with dropout resembles training an ensemble of models. The models automatically follow the same layer structure and share neuron parameters. The number of possible models indirectly trained with dropout is roughly exponential in the number of non-output units $2^{||N||}$. Dropout prohibits only model graphs with broken connection between input and output units. Of course, these models are not trained explicitly and only a small subset is used during the training process. For training with dropout we can update optimization problem \ref{eq:optnn} as follows:

\begin{equation}\label{eq:optnndo}
  \hat{\theta} = \argmin_{\theta \in \Bbb{R}^D} \E_\mu J(\theta, \mu) = \argmin_{\theta \in \Bbb{R}^D} \E_\mu \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i, \theta, \mu))
\end{equation}
where $\mu$ is a random mask with $\{1, 0\}$ mask values for each non-output neuron. We have to provide \textit{keep probability} value $p_{keep} \in (0, 1]$. Keep probability specifies how likely each neuron to participate in every single training step. In terms of mask $\mu$ $p_{keep}=p(\mu_i = 1)$.

This way during the training process we minimize the value of $\E_\mu J(\theta, \mu)$ iteratively sampling masks $\mu$ and minimizing function $J(\theta, \mu)$ for this masks explicitly.

To perform prediction with such a model we need to collect votes from an exponential number of ensemble members.
In practice the following two methods are commonly used. First, alike during the training process, we can sample some constant number $N$ of masks $\mu$ and use majority vote on a small subset of functions $\{f(x, \theta, \mu_1), \ldots, f(x, \theta, \mu_N)\}$. Alternatively, it has been shown that simply scaling the weight of the layers by \textit{keep probability} provides a good approximation of the stochastic evaluation \cite{Srivastava2014}. Later method requires only a single forward pass through the complete network for each prediction.

At last, dropout becomes especially useful when it is hard to control the actual model complexity. Developing or even fine-tuning a new task specific neural network architecture is a demanding process. It becomes more common to reuse an existing network architecture as Inception or ResNet \cite{He2015, Szegedy2016}. Even though number of parameters in this networks can be overwhelming for a particular task, rigorous regularization with dropout might be sufficient to achieve satisfactory results.


\section{Autoencoders}\label{ch:ae}
An autoencoder or an autoassociator neural network is an unsupervised learning algorithm that sets target output values equal to input values $y_i=x_i$ \cite{Ng2011,RanzatoMarcAurelio2007}.
Autoencoders are usually trained according to the \textit{encoder-decoder} paradigm.
Which first allows to project or encode input $x_i$ into some useful feature representation $h_i$.
Then reconstruct or decode the original input $\hat{y_i}$ from representation $h_i$.
This schematically depicted on figure \ref{fig:ae}.


\input{auxilary/ae}


We are going to refer to encoder as a deterministic differentiable function $f(x, \theta_f)$ that for a given set of parameters $\theta_f$ maps input $x\in \Bbb{R}^d$ into representation $h \in \Bbb{R}^{d'}$.
Likewise, the decoder is a deterministic differentiable function $g(h, \theta_g)$ depending on parameters $\theta_g$, that maps $h\in\Bbb{R}^{d'}$ into $y\in \Bbb{R}^d$. We will further avoid specifying parameters $\theta_f$ and $\theta_g$ for simplicity.
For a specific loss function $L$ and a dataset $X=\{x_0, ..., x_i\}$ we can define the learning objective as follows \cite{Good2016}:

\begin{equation}\label{eq:ae}
\min_{\theta_f, \theta_g}\sum\limits_{x_i \in X}{L(x_i, g(f(x))}
\end{equation}

Learning to reconstruct the input information from input information is not particulaly useful by itself.
Instead we are often interested in reusing only parts of the autoencoder.
For example, parameters of the encoder $\theta_f$ can serve as a good initialization of a discriminative model \cite{Masci2011, Vincent2010, Zhao2015}.
Such initialization has shown to lead to minor yet coherent improvement of generalization properties of the model.
Variational Autoencoders, on the other hand, reuse decoder as a generator of new training examples \cite{Kingma2013}.
Extracted features $h$ can represent interpretable properties of the input and can be used to adjust this
properties during generation process \cite{Kulkarni2015, Whitney2016}.

Learning useful feature representations is not a trivial task.
One of the common issues with training autoencoders is the possibility to learn an identity mapping of input data.
Learning an identity mapping would result in less informative feature representation of the inputs.
For example, we can represent both encoder and decoder as a linear function and set the size of the feature space so that it exceeds the size of input space $d' > d$.
Then components of autoencoder can both learn simple identity mappings from input space into feature space and vise versa.
Therefore, it is common to either limit the capacity of the feature space to be less than the input space or impose additional regularization on the extracted features.

Additionally, both encoder and decoder can be represented as arbitrary neural networks.
If the capacity of any of the two networks is high enough, it would again lead to learning of an identity mapping.
On the other hand, if the capacity is chosen too low it can hurt the learning process.
For example, low capacity autoencoders are prone to \textit{mode collapse}, when model produces the same output regardless of the input data \cite{Radford2015}.

One of the ways to address the problem of overly high capacity of components can be using stacked autoencoders.
In this approach, instead of directly mapping $h=f(x)$ and $y=g(h)$ into desired lower-dimensional space multiple intermediate mappings can be used $h_i=f_i(h_{i-1})$ and $\hat{h}_i=g_{i+1}(\hat{h}_{i+1})$.
Stacked autoencoder can be trained in layer-wise manner.
In this case first the mappings $h_1=f_1(x)$  and $y=g_1(h_1)$ are trained.
Then follows training of mappings $h_2=f_1(h_1)$ and $h_1=g_2(h_2)$ and so forth.
The architecture of stacked autoencoder is shown on figure \ref{fig:mlae}
% \ref{fig}

\input{auxilary/mlae}


\subsection{Autoencoders in computer vision}\label{ch:dcae}

In this subsection we discuss unsupervised learning techniques specific to vision tasks.
First, we describe the usage of convolutional layers in autoencoders \ref{ch:cae} and data augmentation \ref{ch:denae}.
Subsection \ref{ch:mod_ae} describes possible modifications of learning objective in autoencoders.

Computer vision tasks often have to work with noisy, highly correlated and very high dimensional data. Input size can grow to thousands of features per input example \ref{ILSVRC15}.
Computer vision models have to account for highly nonlinear relation between input features.
Learned concepts have to be highly tolerant to the position of the features on the image grid and be capable to work with spatially transformed and noisy data.
This requirements generally lead to increase in model complexity.

In this subsection we discuss the means of increasing generalization properties of complex computer vision models and modification, more specific to autoencoders.

\subsubsection{Denoising autoencoders}\label{ch:denae}

Learning high capacity models is prone to overfitting.
One of the common ways to decrease overfitting and make model learn useful features without actually changing the model is learning on a larger training corpus.

Denoising autoencoders \cite{Vincent2010} allow to increase variation in training data by adding random noise to visual data. We construct input examples by adding random noise to the dataset according to some parameter $alpha$: $\hat{x}=n(x, \alpha)$.
Denoising autoencoders are yet supposed to reconstruct  the original image $L(x, g(f(x)))$.
This approach expects encoded representation to be stable and robust under condition of input corruption.
It also expects that the denoising tasks would result in learning useful structure of the input distribution.
Denoising autoencoders are reported to allow better generalization, when an encoded network is used as initializer for the classifier.

\subsubsection{Convolutional autoencoders}\label{ch:cae}

One of the issues commonly encountered in computer vision tasks is spatial correlation of the data.
Neighboring pixels of a single image are rarely fully independent.
Fully-connected networks are ill-fitted for addressing such local correlations of the data.
Furthermore, high dimensionality of computer vision data results in over-parametrization of such models and leads to overfitting.
Recent trends are clearly skewed towards convolutional models \cite{He2015, Szegedy2016} that are consistently showing the best results in computer vision competitions \cite{ILSVRC15, Zhou2016}.

Convolutional and max pooling layers provide means of dimensionality reduction.
As described in section \ref{ch:cnn}, applying strided convolutions or pooling layers leads to a decrease in resolution of the feature map.
Therefore convolutional and max pooling layers are natural candidates for feature extraction in encoder network.

Convolutional networks depend on a relatively small number of parameters.
This quality is desirable for learning a useful feature mapping since it allows to avoid overfitting and, as a result, learning of an identity mapping by the network.
As described in section \ref{ch:cnn}, number of parameters of convolutional layer depends only on kernel size and depth of the feature map.
The number of parameters required by pooling layers is relatively small if not zero and does not depend on the size of the input.

\subsubsection{Deconvolutional layers and input reconstruction}

While convolutional layers are natural for feature extraction in the encoder, reconstruction of highly dimensional input requires a different technique.
One approach is to use an inverse-like operation to convolution.
Deconvolution layer, also known as transposed convolution, is one possible approach.

Deconvolutional layers \cite{Zeiler2010} take as an input an image $y$ composed of $K_0$ color channels $y_1, ... , y_{K_0}$.
As a result of the deconvolution process we would like to obtain $K_1$ feature maps of the same image.
We can represent each channel $c \in [1, \ldots, K_0]$ as $K_1$ channels convolved with feature maps $f_{k,c}$ \ref{eq:conv}:

\begin{equation}\label{eq:de}
  \sum^{K_1}_{k=1}=z^i_k \cdot f_{k,c} = y^i_{k,c}
\end{equation}

where $\cdot$ is a dot product.

Using this equation we would like to find latent feature maps $f_{k,c}$.
Since equation \ref{eq:de} is under-determined and allows multiple solutions.
Therefore a regularization term $z^i_k$ is added to encourage sparsity of the solutions.
By adding a sparsity term we can express full cost function $C$ for input $y_i$ as follows:

\begin{equation}\label{eq:dec}
    C(y_i) = \frac{\lambda}{2} \sum^{K_o}_{c=1} ||\sum^{K_1}_{k=1}{z^i_k \cdot f_{k,c} - y^i_{k,c}}||^2_2 + \sum^{K_1}_{k=1}{|z^i_k|^p}
\end{equation}
where we assume mean square error as a reconstruction cost and $p$ as regularization norm.
Constant $\lambda$ balances contributions of the regularization term to the cost function.

Problem \ref{eq:dec} for simplicity can be decomposed into 2 sequential quadratic problems and solved using the stochastic gradient descent method seamlessly with backpropagation.

\subsubsection{Unpooling layers}

Like with convolutional layers, applying pooling layers might results in change of the shape of the feature map.
In this subsection we describe a method that allows to effectively revert max-pooling layer preserving the relative spatial position of the extracted features as on original image.

In section \ref{ch:cnn} we described applying max pooling layer with some stride $k$.
Applying these layers allows to reduce the size of the feature map up to $k^2$ times while preserving, arguably, most relevant information about the extracted features.
Max-pooling is crucial to convolutional neural networks for achieving some position tolerance of the extracted features \cite{Jaderberg2015}.

The inverse operation of max-pooling is not well defined, since the maximum value must be placed somewhere inside the original volume of size $k^2$. Naively, we can \textit{upsample} the features by simply putting maximum values into a fixed position within each upsample volume $k \times k$.
Upsampling procedure is depicted on figure \ref{fig:pool}.
However, this approach destroys relative positioning of the features.
In other words, network might try to reconstruct 'nose' and 'eye' features in exactly the same region instead of placing them next to each other.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{upsample.png}
  \caption{Upsampling image after max-pooling by placing max value into fixed grid position. Method described by \cite{Dosovitskiy2015a}.}
  \label{fig:pool}
\end{figure}

To address this issue an unpooling layer is introduced. Instead of using information about only the maximum value of the feature in the region, unpooling layers also use information about positioning of the feature. This positional information is called a \textit{mask}. To every pooled value $x_{i,j}$ corresponds a single mask, indicating the position of the value $x_{i,j}$ inside the pulled volume. We can descripe unpooling operation $up(X, M)$ as follows:

\begin{equation}\label{eq:unp}
  \begin{aligned}
    & Y = up(X, M),\text{ such that} \\
    & \quad X \in \Bbb{R}^{W \times H \times C}, \\
    & \quad Y \in \Bbb{R}^{kW \times kH \times C}, \text{ where $k$ is stride } \\
    & \quad M \in \Bbb{J}^{W \times H \times C}, \text{ where $\Bbb{J}$ is a set of size $|\Bbb{J}|=k^2$ }
  \end{aligned}
\end{equation}

As follows, pooling layers described in section \ref{ch:cnn} must also extract the information $M$ about the positions of maximum values. This modification does not otherwise effect the layer neither during forward nor during backward pass. Application of unpooling layers is illustrated on figure \ref{fig:unpool}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{unpool.png}
  \caption{Illustration of pooling and unpooling layers.}
  \label{fig:unpool}
\end{figure}

Unpooling layers contain no parameters and are not affected by backpropagation.

It's worth noting, that mask information extracted by a modified pooling layer depends on the order of feature channels. As stated by the authors of the original idea, this might have an effect on features, extracted by layers preceding unpooling \cite{Zhao2015}.

\subsubsection{What-Where autoencoders}



\subsection{Extracting useful features with Autoencoders}\label{ch:mod_ae}

With the autoencoder paradigm we try to extract useful features from the input data. In this subsection we try to disambiguate the term \textit{useful} and, show how it can be achieved in context of several learning tasks.

\subsubsection{Sparse Autoencoders}\label{ch:sae}

Lets consider activations in the hidden layer $h$ for some input $x$:
\begin{equation}
  h(x) = \sigma(W_{h}x_{-1} + b_{h})
\end{equation}
where $W_h$ and $b_h$ are parameters of a hidden layer $h$, $x_{-1}$ is the output of the previous hidden layer and $\sigma$ is some activation function.
Sparse autoencoders introduce an additional term $l_{sparse}$ to the loss function to add a constrain on the activations of the hidden layer $h$ \cite{Ng2011}:
\begin{equation}
  l_{total}(x) = l_{reconstruction}(x, g(f(x))) + \alpha*l_{sparse}(x, h(x))
\end{equation}
where $\alpha$ is a constant parameter.

A sparsity constrain tries to achieve low average activation $\hat{\rho}$ on the hidden layer:

\begin{equation}\label{eq:avgh}
  \rho = \frac{1}{N} \sum_{i=1}^N h_i(x)
\end{equation}

For example, let's take a look at the case for sigmoid activation function in the hidden layer.
Sigmoid produces outputs in the interval $[0, 1]$. If we can enforce an average activation on the hidden layer $\hat{\rho} \leq 0.1$ we can rely upon the fact, that no more than $10\%$ of hidden neurons would have activation close to $1.0$.

A sparse constrain allows to extract useful features even for relatively high number of hidden units. Specifically, a decoder must be able to reconstruct the original representation relying only on few high activation on the hidden layer.

There are several ways to enforce a sparsity constrain.
It is possible to perform gradient descent directly for fixed value of $\hat{\rho}$. In that case we would have to perform a second pass over the dataset to calculate the actual value of $\rho$ to be used as a target during learning with gradient descent. Instead we can simply nullify all but the $k$ highest activation during the forward pass through the network \cite{Kulkarni2015}.
Or apply negative log-likelihood to the hidden layer of the network \cite{Zhao2015}.
At last, to achieve true zero values on the hidden layer we can use an absolute value penalty $l_{sparse}=\sum_{i=1}^N |h_i|$ in conjunction with rectifier a linear unit activation function \cite{Glorot2011}.

Sparse coding of the data has proven to be useful to solve the classification problem.
In particular, the classification task requires us to assign one of $C$ exclusive classes we can train an autoencoder with $C$ hidden units. Once training converges we expect that features, learned by the hidden layer would somewhat correspond to classes of tasks. We can then use the encoder part of the network as initialization of a classifier of the same architecture \cite{Masci2011}.
Such initialization leads to consistently better generalization results after supervised training of the classifier.

\subsubsection{Variational Autoencoders}\label{ch:vae}



% \subsection{Generative adversarial nets}\label{ch:gan}
% \section{Autoencoders and feature interpretability}
